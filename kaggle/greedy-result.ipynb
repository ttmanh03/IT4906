{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5c394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.078079Z",
     "iopub.status.busy": "2025-11-23T18:27:14.077669Z",
     "iopub.status.idle": "2025-11-23T18:27:14.083183Z",
     "shell.execute_reply": "2025-11-23T18:27:14.082091Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.078054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecbdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.084807Z",
     "iopub.status.busy": "2025-11-23T18:27:14.084505Z",
     "iopub.status.idle": "2025-11-23T18:27:14.111429Z",
     "shell.execute_reply": "2025-11-23T18:27:14.110364Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.084778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self, space_size=400, r_sen=50, max_cluster_size=20, min_cluster_size=5):\n",
    "        self.space_size = space_size\n",
    "        self.r_sen = r_sen\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "\n",
    "    def estimate_optimal_k(self, nodes, base_station=(200,200,400)):\n",
    "        \"\"\"\n",
    "        ∆Ø·ªõc t√≠nh s·ªë c·ª•m t·ªëi ∆∞u d·ª±a tr√™n c√¥ng th·ª©c WSN\n",
    "        K = sqrt(N*L / (pi*d_tobs))\n",
    "        \"\"\"\n",
    "        N = len(nodes)\n",
    "        base_pos = np.array(base_station)\n",
    "\n",
    "        # Khoang cach trung binh toi base station\n",
    "        distances = np.linalg.norm(nodes - base_pos, axis=1)\n",
    "        d_tobs = np.mean(distances)\n",
    "\n",
    "        space_size = self.space_size\n",
    "\n",
    "        k_optimal = np.sqrt(N * space_size / (np.pi * d_tobs))\n",
    "        k_optimal = max(2, int(np.round(k_optimal)))\n",
    "\n",
    "        # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n max_cluster_size\n",
    "        k_min = int(np.ceil(N / self.max_cluster_size))\n",
    "        k_optimal = max(k_optimal, k_min)\n",
    "        \n",
    "        return k_optimal\n",
    "    \n",
    "    def check_cluster_validity(self, cluster_nodes):\n",
    "        \"\"\"\n",
    "        Kiem tra tinh hop le cua cum\n",
    "        \"\"\"\n",
    "        size = len(cluster_nodes)\n",
    "\n",
    "        # Ki·ªÉm tra k√≠ch th∆∞·ªõc\n",
    "        if size < self.min_cluster_size or size > self.max_cluster_size:\n",
    "            return False, 0, size\n",
    "        \n",
    "        # Ki·ªÉm tra kho·∫£ng c√°ch\n",
    "        if size > 1:\n",
    "            distances = pdist(cluster_nodes)\n",
    "            max_dist = np.max(distances)\n",
    "            \n",
    "            if max_dist > self.r_sen:\n",
    "                return False, max_dist, size\n",
    "            \n",
    "            return True, max_dist, size\n",
    "        \n",
    "        return True, 0, size\n",
    "    \n",
    "    def split_invalid_cluster(self, cluster_nodes, cluster_ids):\n",
    "        \"\"\"\n",
    "        Chia nh·ªè c·ª•m kh√¥ng h·ª£p l·ªá th√†nh c√°c c·ª•m con\n",
    "        \"\"\"\n",
    "        # N·∫øu c·ª•m ch·ªâ c√≥ 1 node, kh√¥ng th·ªÉ chia\n",
    "        if len(cluster_nodes) < 2:\n",
    "            return [(cluster_nodes, cluster_ids)]\n",
    "        \n",
    "        # S·ª≠ d·ª•ng K-Means ƒë·ªÉ chia 2\n",
    "        kmeans = KMeans(n_clusters=2, n_init=20, random_state=42)\n",
    "        labels = kmeans.fit_predict(cluster_nodes)\n",
    "        \n",
    "        sub_clusters = []\n",
    "        for i in range(2):\n",
    "            sub_nodes = cluster_nodes[labels == i]\n",
    "            sub_ids = [cluster_ids[j] for j in range(len(cluster_ids)) if labels[j] == i]\n",
    "            \n",
    "            if len(sub_nodes) > 0:\n",
    "                sub_clusters.append((sub_nodes, sub_ids))\n",
    "        \n",
    "        return sub_clusters\n",
    "    \n",
    "    def merge_small_clusters(self, clusters_data):\n",
    "        \"\"\"\n",
    "        G·ªôp c√°c c·ª•m nh·ªè v·ªõi c·ª•m l√°ng gi·ªÅng g·∫ßn nh·∫•t\n",
    "        \"\"\"\n",
    "        if len(clusters_data) <= 1:\n",
    "            return clusters_data\n",
    "        \n",
    "        merged = []\n",
    "        to_merge = []\n",
    "        \n",
    "        # T√¨m c√°c c·ª•m nh·ªè\n",
    "        for nodes, ids in clusters_data:\n",
    "            if len(nodes) < self.min_cluster_size:\n",
    "                to_merge.append((nodes, ids))\n",
    "            else:\n",
    "                merged.append((nodes, ids))\n",
    "        \n",
    "        # G·ªôp t·ª´ng c·ª•m nh·ªè v√†o c·ª•m g·∫ßn nh·∫•t\n",
    "        for small_nodes, small_ids in to_merge:\n",
    "            if len(merged) == 0:\n",
    "                merged.append((small_nodes, small_ids))\n",
    "                continue\n",
    "            \n",
    "            # T√¨m c·ª•m g·∫ßn nh·∫•t\n",
    "            small_center = np.mean(small_nodes, axis=0)\n",
    "            min_dist = float('inf')\n",
    "            best_idx = 0\n",
    "            \n",
    "            for i, (nodes, ids) in enumerate(merged):\n",
    "                center = np.mean(nodes, axis=0)\n",
    "                dist = np.linalg.norm(small_center - center)\n",
    "                \n",
    "                # Ki·ªÉm tra xem g·ªôp c√≥ v∆∞·ª£t qu√° max_size kh√¥ng\n",
    "                if dist < min_dist and len(nodes) + len(small_nodes) <= self.max_cluster_size:\n",
    "                    min_dist = dist\n",
    "                    best_idx = i\n",
    "            \n",
    "            # G·ªôp\n",
    "            merged[best_idx] = (\n",
    "                np.vstack([merged[best_idx][0], small_nodes]),\n",
    "                merged[best_idx][1] + small_ids\n",
    "            )\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def cluster_with_constraints(self, nodes, node_ids, k=None, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Ph√¢n c·ª•m v·ªõi r√†ng bu·ªôc - Thu·∫≠t to√°n ch√≠nh\n",
    "        \n",
    "        Args:\n",
    "            nodes: T·ªça ƒë·ªô 3D c·ªßa nodes\n",
    "            node_ids: ID c·ªßa nodes\n",
    "            k: S·ªë c·ª•m (n·∫øu None s·∫Ω t·ª± ƒë·ªông ∆∞·ªõc t√≠nh)\n",
    "            max_iterations: S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa ƒë·ªÉ ƒëi·ªÅu ch·ªânh\n",
    "            \n",
    "        Returns:\n",
    "            List of (cluster_nodes, cluster_ids)\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.estimate_optimal_k(nodes)\n",
    "        \n",
    "        print(f\"B·∫Øt ƒë·∫ßu ph√¢n c·ª•m v·ªõi k={k}\")\n",
    "        \n",
    "        # B∆∞·ªõc 1: K-Means ban ƒë·∫ßu\n",
    "        kmeans = KMeans(n_clusters=k, n_init=30, random_state=42)\n",
    "        labels = kmeans.fit_predict(nodes)\n",
    "        \n",
    "        # B∆∞·ªõc 2: T·∫°o c√°c c·ª•m v√† ki·ªÉm tra\n",
    "        iteration = 0\n",
    "        while iteration < max_iterations:\n",
    "            print(f\"  V√≤ng l·∫∑p {iteration + 1}/{max_iterations}\")\n",
    "            \n",
    "            valid_clusters = []\n",
    "            invalid_clusters = []\n",
    "            \n",
    "            # Ph√¢n lo·∫°i c·ª•m h·ª£p l·ªá v√† kh√¥ng h·ª£p l·ªá\n",
    "            for i in range(k):\n",
    "                cluster_nodes = nodes[labels == i]\n",
    "                cluster_ids = [node_ids[j] for j in range(len(node_ids)) if labels[j] == i]\n",
    "                \n",
    "                if len(cluster_nodes) == 0:\n",
    "                    continue\n",
    "                \n",
    "                is_valid, max_dist, size = self.check_cluster_validity(cluster_nodes)\n",
    "                \n",
    "                if is_valid:\n",
    "                    valid_clusters.append((cluster_nodes, cluster_ids))\n",
    "                    print(f\"    C·ª•m {i}: ‚úì h·ª£p l·ªá (size={size}, max_dist={max_dist:.1f}m)\")\n",
    "                else:\n",
    "                    invalid_clusters.append((cluster_nodes, cluster_ids))\n",
    "                    print(f\"    C·ª•m {i}: ‚úó kh√¥ng h·ª£p l·ªá (size={size}, max_dist={max_dist:.1f}m)\")\n",
    "            \n",
    "            # N·∫øu t·∫•t c·∫£ h·ª£p l·ªá, k·∫øt th√∫c\n",
    "            if len(invalid_clusters) == 0:\n",
    "                print(f\"  ‚Üí T·∫•t c·∫£ c·ª•m h·ª£p l·ªá!\")\n",
    "                break\n",
    "            \n",
    "            # B∆∞·ªõc 3: X·ª≠ l√Ω c√°c c·ª•m kh√¥ng h·ª£p l·ªá\n",
    "            for cluster_nodes, cluster_ids in invalid_clusters:\n",
    "                size = len(cluster_nodes)\n",
    "                \n",
    "                if size > self.max_cluster_size:\n",
    "                    # C·ª•m qu√° l·ªõn ‚Üí Chia nh·ªè\n",
    "                    print(f\"    ‚Üí Chia c·ª•m (size={size})\")\n",
    "                    sub_clusters = self.split_invalid_cluster(cluster_nodes, cluster_ids)\n",
    "                    valid_clusters.extend(sub_clusters)\n",
    "                else:\n",
    "                    # C·ª•m c√≥ kho·∫£ng c√°ch qu√° l·ªõn ‚Üí Chia nh·ªè\n",
    "                    print(f\"    ‚Üí Chia c·ª•m (kho·∫£ng c√°ch l·ªõn)\")\n",
    "                    sub_clusters = self.split_invalid_cluster(cluster_nodes, cluster_ids)\n",
    "                    valid_clusters.extend(sub_clusters)\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t labels v√† k cho v√≤ng l·∫∑p ti·∫øp theo\n",
    "            k = len(valid_clusters)\n",
    "            \n",
    "            # T·∫°o l·∫°i labels t·ª´ valid_clusters\n",
    "            labels = np.zeros(len(nodes), dtype=int)\n",
    "            for cluster_idx, (_, cluster_ids) in enumerate(valid_clusters):\n",
    "                for node_id in cluster_ids:\n",
    "                    node_idx = node_ids.index(node_id)\n",
    "                    labels[node_idx] = cluster_idx\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # B∆∞·ªõc 4: G·ªôp c√°c c·ª•m qu√° nh·ªè\n",
    "        valid_clusters = self.merge_small_clusters(valid_clusters)\n",
    "        \n",
    "        print(f\"Ho√†n th√†nh: {len(valid_clusters)} c·ª•m\")\n",
    "        return valid_clusters\n",
    "    \n",
    "    def choose_cluster_head(self, cluster_nodes, cluster_ids, node_data=None):\n",
    "        \"\"\"\n",
    "        Ch·ªçn cluster head\n",
    "        - ∆Øu ti√™n: Node c√≥ nƒÉng l∆∞·ª£ng cao nh·∫•t\n",
    "        - D·ª± ph√≤ng: Node g·∫ßn t√¢m c·ª•m nh·∫•t\n",
    "        \"\"\"\n",
    "        if node_data:\n",
    "            # Ch·ªçn theo nƒÉng l∆∞·ª£ng\n",
    "            max_energy = -1\n",
    "            ch_id = cluster_ids[0]\n",
    "            \n",
    "            for nid in cluster_ids:\n",
    "                if nid in node_data and 'residual_energy' in node_data[nid]:\n",
    "                    energy = node_data[nid]['residual_energy']\n",
    "                    if energy > max_energy:\n",
    "                        max_energy = energy\n",
    "                        ch_id = nid\n",
    "            \n",
    "            return ch_id\n",
    "        else:\n",
    "            # Ch·ªçn theo kho·∫£ng c√°ch ƒë·∫øn t√¢m\n",
    "            center = np.mean(cluster_nodes, axis=0)\n",
    "            distances = np.linalg.norm(cluster_nodes - center, axis=1)\n",
    "            min_idx = np.argmin(distances)\n",
    "            return cluster_ids[min_idx]\n",
    "    \n",
    "    def calculate_metrics(self, clusters_data):\n",
    "        \"\"\"\n",
    "        T√≠nh c√°c metric ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'num_clusters': len(clusters_data),\n",
    "            'avg_cluster_size': 0,\n",
    "            'min_cluster_size': float('inf'),\n",
    "            'max_cluster_size': 0,\n",
    "            'avg_intra_distance': 0,\n",
    "            'max_intra_distance': 0,\n",
    "            'balance_score': 0  # ƒê·ªô c√¢n b·∫±ng k√≠ch th∆∞·ªõc c·ª•m (0-1, c√†ng cao c√†ng t·ªët)\n",
    "        }\n",
    "        \n",
    "        sizes = []\n",
    "        intra_dists = []\n",
    "        \n",
    "        for nodes, ids in clusters_data:\n",
    "            size = len(nodes)\n",
    "            sizes.append(size)\n",
    "            \n",
    "            metrics['min_cluster_size'] = min(metrics['min_cluster_size'], size)\n",
    "            metrics['max_cluster_size'] = max(metrics['max_cluster_size'], size)\n",
    "            \n",
    "            if size > 1:\n",
    "                distances = pdist(nodes)\n",
    "                intra_dists.append(np.mean(distances))\n",
    "                metrics['max_intra_distance'] = max(metrics['max_intra_distance'], np.max(distances))\n",
    "        \n",
    "        metrics['avg_cluster_size'] = np.mean(sizes)\n",
    "        metrics['avg_intra_distance'] = np.mean(intra_dists) if intra_dists else 0\n",
    "        \n",
    "        # T√≠nh balance score (d·ª±a tr√™n coefficient of variation)\n",
    "        cv = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 0\n",
    "        metrics['balance_score'] = 1 / (1 + cv)  # 1 = ho√†n to√†n c√¢n b·∫±ng\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7ebeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.112497Z",
     "iopub.status.busy": "2025-11-23T18:27:14.112227Z",
     "iopub.status.idle": "2025-11-23T18:27:14.130428Z",
     "shell.execute_reply": "2025-11-23T18:27:14.129599Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.112478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    Lx, Ly, Lz = x2 - x1, y2 - y1, z2 - z1\n",
    "    L_mag = math.sqrt(Lx**2 + Ly**2 + Lz**2)\n",
    "    if L_mag == 0:\n",
    "        return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta) / v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    if abs(cos_beta) < 1e-9:\n",
    "        return v_AUV\n",
    "    return abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    total_time = 0.0\n",
    "    if len(path) <= 1:\n",
    "        return 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i + 1]]\n",
    "        d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        total_time += d / max(v_s, 1e-9)\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    total_time += d / max(v_s, 1e-9)\n",
    "    return total_time\n",
    "\n",
    "def path_distance(path, coords):\n",
    "    if len(path) <= 1:\n",
    "        return 0.0\n",
    "    dist = 0.0\n",
    "    for i in range(len(path)-1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i+1]]\n",
    "        dist += np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    dist += np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21183c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_and_bound_path(centers, v_f, v_AUV):\n",
    "    \"\"\"Thu·∫≠t to√°n Nh√°nh v√† C·∫≠n (Branch and Bound) t√¨m ƒë∆∞·ªùng ƒëi t·ªëi ∆∞u to√†n c·ª•c.\n",
    "    centers: list t·ªça ƒë·ªô, index 0 l√† base station.\n",
    "    v_f: v·∫≠n t·ªëc d√≤ng ch·∫£y; v_AUV: v·∫≠n t·ªëc AUV g·ªëc.\n",
    "    Tr·∫£ v·ªÅ list index theo th·ª© t·ª± thƒÉm t·ªëi ∆∞u (bao g·ªìm quay v·ªÅ base station).\"\"\"\n",
    "    import heapq\n",
    "    \n",
    "    n = len(centers)\n",
    "    if n <= 1:\n",
    "        return [0]\n",
    "    \n",
    "    pts = np.array(centers)\n",
    "    \n",
    "    # T√≠nh ma tr·∫≠n th·ªùi gian di chuy·ªÉn gi·ªØa c√°c ƒëi·ªÉm\n",
    "    def calc_travel_time(i, j):\n",
    "        p_i = tuple(pts[i])\n",
    "        p_j = tuple(pts[j])\n",
    "        dist = np.linalg.norm(pts[j] - pts[i])\n",
    "        v_s = compute_vs(p_i, p_j, v_f, v_AUV)\n",
    "        return dist / max(v_s, 1e-9)\n",
    "    \n",
    "    time_matrix = [[0.0] * n for _ in range(n)]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                time_matrix[i][j] = calc_travel_time(i, j)\n",
    "    \n",
    "    # T√≠nh c·∫≠n d∆∞·ªõi (lower bound) s·ª≠ d·ª•ng t·ªïng min outgoing edge\n",
    "    def calculate_lower_bound(path, visited):\n",
    "        lb = 0\n",
    "        # Chi ph√≠ ƒë∆∞·ªùng ƒëi hi·ªán t·∫°i\n",
    "        for i in range(len(path) - 1):\n",
    "            lb += time_matrix[path[i]][path[i + 1]]\n",
    "        \n",
    "        # ∆Ø·ªõc t√≠nh chi ph√≠ t·ªëi thi·ªÉu cho c√°c node ch∆∞a thƒÉm\n",
    "        last = path[-1]\n",
    "        unvisited = [i for i in range(n) if i not in visited]\n",
    "        \n",
    "        if unvisited:\n",
    "            # Chi ph√≠ t·ªëi thi·ªÉu t·ª´ node hi·ªán t·∫°i ƒë·∫øn m·ªôt node ch∆∞a thƒÉm\n",
    "            min_from_last = min(time_matrix[last][j] for j in unvisited)\n",
    "            lb += min_from_last\n",
    "            \n",
    "            # Chi ph√≠ t·ªëi thi·ªÉu outgoing t·ª´ m·ªói node ch∆∞a thƒÉm\n",
    "            for node in unvisited:\n",
    "                candidates = [j for j in range(n) if j != node and (j not in visited or j == 0)]\n",
    "                if candidates:\n",
    "                    lb += min(time_matrix[node][j] for j in candidates)\n",
    "        else:\n",
    "            # T·∫•t c·∫£ ƒë√£ thƒÉm - th√™m chi ph√≠ quay v·ªÅ base station\n",
    "            lb += time_matrix[last][0]\n",
    "        \n",
    "        return lb\n",
    "    \n",
    "    # Nearest neighbor ƒë·ªÉ c√≥ upper bound ban ƒë·∫ßu (c√≥ t√≠nh quay v·ªÅ)\n",
    "    def nearest_neighbor_heuristic():\n",
    "        unvisited = set(range(1, n))\n",
    "        path = [0]\n",
    "        cur = 0\n",
    "        total_time = 0\n",
    "        while unvisited:\n",
    "            best_next = min(unvisited, key=lambda x: time_matrix[cur][x])\n",
    "            total_time += time_matrix[cur][best_next]\n",
    "            path.append(best_next)\n",
    "            unvisited.remove(best_next)\n",
    "            cur = best_next\n",
    "        # Th√™m th·ªùi gian quay v·ªÅ base station\n",
    "        total_time += time_matrix[cur][0]\n",
    "        return path, total_time\n",
    "    \n",
    "    best_path, best_time = nearest_neighbor_heuristic()\n",
    "    \n",
    "    # Priority queue: (lower_bound, path, visited_set)\n",
    "    # S·ª≠ d·ª•ng tuple ƒë·ªÉ c√≥ th·ªÉ so s√°nh\n",
    "    initial_visited = frozenset([0])\n",
    "    initial_lb = calculate_lower_bound([0], initial_visited)\n",
    "    \n",
    "    # heap entries: (lower_bound, counter, path, visited)\n",
    "    counter = 0\n",
    "    heap = [(initial_lb, counter, [0], initial_visited)]\n",
    "    \n",
    "    while heap:\n",
    "        lb, _, current_path, visited = heapq.heappop(heap)\n",
    "        \n",
    "        # Pruning: b·ªè qua n·∫øu lower bound >= best known\n",
    "        if lb >= best_time:\n",
    "            continue\n",
    "        \n",
    "        current_node = current_path[-1]\n",
    "        \n",
    "        # N·∫øu ƒë√£ thƒÉm t·∫•t c·∫£ c√°c node\n",
    "        if len(current_path) == n:\n",
    "            # T√≠nh t·ªïng th·ªùi gian (C√ì quay v·ªÅ base station)\n",
    "            total_time = sum(time_matrix[current_path[i]][current_path[i + 1]] \n",
    "                           for i in range(len(current_path) - 1))\n",
    "            total_time += time_matrix[current_path[-1]][0]  # Quay v·ªÅ base station\n",
    "            if total_time < best_time:\n",
    "                best_time = total_time\n",
    "                best_path = current_path[:]\n",
    "            continue\n",
    "        \n",
    "        # M·ªü r·ªông c√°c node ch∆∞a thƒÉm\n",
    "        for next_node in range(n):\n",
    "            if next_node not in visited:\n",
    "                new_path = current_path + [next_node]\n",
    "                new_visited = visited | frozenset([next_node])\n",
    "                new_lb = calculate_lower_bound(new_path, new_visited)\n",
    "                \n",
    "                # Pruning\n",
    "                if new_lb < best_time:\n",
    "                    counter += 1\n",
    "                    heapq.heappush(heap, (new_lb, counter, new_path, new_visited))\n",
    "    \n",
    "    return best_path\n",
    "\n",
    "\n",
    "def nearest_neighbor_path(centers, v_f, v_AUV):\n",
    "    \"\"\"Wrapper function - s·ª≠ d·ª•ng Branch and Bound thay v√¨ Nearest Neighbor.\n",
    "    Gi·ªØ t√™n h√†m c≈© ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi code hi·ªán t·∫°i.\"\"\"\n",
    "    return branch_and_bound_path(centers, v_f, v_AUV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e739a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.162724Z",
     "iopub.status.busy": "2025-11-23T18:27:14.162201Z",
     "iopub.status.idle": "2025-11-23T18:27:14.179942Z",
     "shell.execute_reply": "2025-11-23T18:27:14.178976Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.162697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_energy(best_time, n_members):\n",
    "    \"\"\"\n",
    "    T√≠nh nƒÉng l∆∞·ª£ng ti√™u th·ª• cho Member Node v√† Cluster Head.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    - n_members: S·ªë l∆∞·ª£ng node th√†nh vi√™n th·ª±c t·∫ø trong cluster (kh√¥ng t√≠nh cluster head)\n",
    "    \"\"\"\n",
    "    G, L = 100, 1024\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Member Node\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Cluster Head (nh·∫≠n t·ª´ n_members node, truy·ªÅn cho AUV)\n",
    "    E_rx_TN = G * P_r * L * n_members / DR\n",
    "    E_tx_TN = G * P_t * L * n_members / DR_i\n",
    "    E_idle_TN = (best_time - (G*L*n_members/DR) - (G*L*n_members/DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "\n",
    "    return {\n",
    "        \"Member\": {\"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "def update_energy(all_nodes, clusters, best_time):\n",
    "    \"\"\"\n",
    "    C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng cho t·∫•t c·∫£ c√°c node d·ª±a tr√™n s·ªë member th·ª±c t·∫ø c·ªßa t·ª´ng cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary ch·ª©a th√¥ng tin t·∫•t c·∫£ c√°c node\n",
    "    - clusters: Dictionary ch·ª©a th√¥ng tin c√°c cluster\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    \"\"\"\n",
    "    for cid, cinfo in clusters.items():\n",
    "        ch = cinfo.get('cluster_head')\n",
    "        nodes = cinfo.get('nodes', [])\n",
    "        \n",
    "        # T√≠nh s·ªë member nodes (kh√¥ng t√≠nh cluster head)\n",
    "        n_members = len([n for n in nodes if n != ch])\n",
    "        \n",
    "        # T√≠nh nƒÉng l∆∞·ª£ng cho cluster n√†y v·ªõi s·ªë member th·ª±c t·∫ø\n",
    "        energy_report = compute_energy(best_time, n_members)\n",
    "        \n",
    "        for nid in nodes:\n",
    "            if nid not in all_nodes: continue\n",
    "            if nid == ch:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Target']['E_total']\n",
    "            else:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Member']['E_total']\n",
    "            all_nodes[nid]['residual_energy'] = max(all_nodes[nid]['residual_energy'], 0.0)\n",
    "\n",
    "def remove_dead_nodes(all_nodes, clusters):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè c√°c node ƒë√£ h·∫øt nƒÉng l∆∞·ª£ng v√† c·∫≠p nh·∫≠t l·∫°i clusters.\n",
    "    \n",
    "    Returns:\n",
    "    - new_clusters: Dictionary c√°c cluster c√≤n node s·ªëng\n",
    "    - dead: List c√°c node_id ƒë√£ ch·∫øt\n",
    "    \"\"\"\n",
    "    dead = [nid for nid, info in list(all_nodes.items()) if info['residual_energy'] <= 0]\n",
    "    for nid in dead:\n",
    "        del all_nodes[nid]\n",
    "\n",
    "    new_clusters = {}\n",
    "    for cid, cinfo in clusters.items():\n",
    "        alive_nodes = [nid for nid in cinfo.get('nodes', []) if nid in all_nodes]\n",
    "        if alive_nodes:\n",
    "            new_c = dict(cinfo)\n",
    "            new_c['nodes'] = alive_nodes\n",
    "            new_clusters[cid] = new_c\n",
    "\n",
    "    return new_clusters, dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f83c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recluster(all_nodes, node_positions, clustering_instance, r_sen=60, max_size=20, min_size=5):\n",
    "    \"\"\"\n",
    "    Ph√¢n c·ª•m l·∫°i to√†n b·ªô c√°c node c√≤n s·ªëng s·ª≠ d·ª•ng thu·∫≠t to√°n t·ª´ cluster.py.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary c√°c node c√≤n s·ªëng\n",
    "    - node_positions: Dictionary v·ªã tr√≠ c·ªßa c√°c node\n",
    "    - clustering_instance: Instance c·ªßa class Clustering\n",
    "    - r_sen: Ng∆∞·ª°ng kho·∫£ng c√°ch t·ªëi ƒëa trong c·ª•m\n",
    "    - max_size: S·ªë l∆∞·ª£ng node t·ªëi ƒëa trong 1 c·ª•m\n",
    "    - min_size: S·ªë l∆∞·ª£ng node t·ªëi thi·ªÉu trong 1 c·ª•m\n",
    "    \n",
    "    Returns:\n",
    "    - clusters: Dictionary c√°c c·ª•m m·ªõi\n",
    "    \"\"\"\n",
    "    ids = sorted(list(all_nodes.keys()))\n",
    "    if len(ids) == 0:\n",
    "        return {}\n",
    "    coords = np.array([node_positions[nid] for nid in ids])\n",
    "    clustering_instance.r_sen = r_sen\n",
    "    clustering_instance.max_cluster_size = max_size\n",
    "    clustering_instance.min_cluster_size = min_size\n",
    "    clusters_data = clustering_instance.cluster_with_constraints(coords, ids)\n",
    "    clusters = {}\n",
    "    for i, (cluster_nodes, cluster_ids) in enumerate(clusters_data):\n",
    "        center = np.mean(cluster_nodes, axis=0).tolist()\n",
    "        ch = clustering_instance.choose_cluster_head(cluster_nodes, cluster_ids, all_nodes)\n",
    "        clusters[i] = {'nodes': cluster_ids, 'center': center, 'cluster_head': ch}\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b93f0-bb87-4303-a031-ef3c71222f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.200339Z",
     "iopub.status.busy": "2025-11-23T18:27:14.199982Z",
     "iopub.status.idle": "2025-11-23T18:27:14.217069Z",
     "shell.execute_reply": "2025-11-23T18:27:14.216159Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.200310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reselect_cluster_heads(clusters, all_nodes):\n",
    "    \"\"\"\n",
    "    Ch·ªâ ch·ªçn l·∫°i cluster head cho c√°c c·ª•m hi·ªán t·∫°i d·ª±a tr√™n nƒÉng l∆∞·ª£ng.\n",
    "    Kh√¥ng ph√¢n c·ª•m l·∫°i.\n",
    "    \"\"\"\n",
    "    for cid, cinfo in clusters.items():\n",
    "        cluster_ids = cinfo['nodes']\n",
    "        # T√¨m node c√≥ nƒÉng l∆∞·ª£ng cao nh·∫•t\n",
    "        max_energy = -1\n",
    "        new_ch = cluster_ids[0]\n",
    "        for nid in cluster_ids:\n",
    "            if nid in all_nodes and 'residual_energy' in all_nodes[nid]:\n",
    "                energy = all_nodes[nid]['residual_energy']\n",
    "                if energy > max_energy:\n",
    "                    max_energy = energy\n",
    "                    new_ch = nid\n",
    "        clusters[cid]['cluster_head'] = new_ch\n",
    "    return clusters\n",
    "\n",
    "print(\"‚úì Helper function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e08e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.218377Z",
     "iopub.status.busy": "2025-11-23T18:27:14.218067Z",
     "iopub.status.idle": "2025-11-23T18:27:14.246263Z",
     "shell.execute_reply": "2025-11-23T18:27:14.245286Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.218350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    M√¥ ph·ªèng tham lam theo th·ªùi gian v·ªõi enriched metadata Option B:\n",
    "    - Reselect CH m·ªói cycle; Recluster + recompute greedy path khi node ch·∫øt.\n",
    "    - Ghi path_stats, clustering metrics, energy summaries, death analytics.\n",
    "    \"\"\"\n",
    "    input_folder = \"/kaggle/input/nodes-data\"\n",
    "    output_folder = \"/kaggle/working/output_data_greedy\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"‚ùå L·ªói: Th∆∞ m·ª•c {input_folder} kh√¥ng t·ªìn t·∫°i!\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "    if len(files) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu n√†o trong {input_folder}\")\n",
    "        return\n",
    "\n",
    "    INITIAL_ENERGY = 100.0\n",
    "    v_f = 1.2\n",
    "    v_AUV = 3.0\n",
    "    R_SEN = 60\n",
    "    MAX_SIZE = 20\n",
    "    MIN_SIZE = 5\n",
    "    \n",
    "    results_summary = []\n",
    "    clustering = Clustering(space_size=400, r_sen=R_SEN, max_cluster_size=MAX_SIZE, min_cluster_size=MIN_SIZE)\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"=== ƒêang x·ª≠ l√Ω file: {filename} (Greedy) ===\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói ƒë·ªçc file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        node_positions = {}\n",
    "        all_nodes = {}\n",
    "        if isinstance(data, list):\n",
    "            for node in data:\n",
    "                nid = node['id']\n",
    "                all_nodes[nid] = {\n",
    "                    'initial_energy': node.get('initial_energy', INITIAL_ENERGY),\n",
    "                    'residual_energy': node.get('residual_energy', INITIAL_ENERGY)\n",
    "                }\n",
    "                node_positions[nid] = (node['x'], node['y'], node['z'])\n",
    "        else:\n",
    "            print(f\"‚ùå C·∫•u tr√∫c file {filename} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£\")\n",
    "            continue\n",
    "\n",
    "        total_nodes = len(all_nodes)\n",
    "        initial_total_energy = sum(nd['initial_energy'] for nd in all_nodes.values())\n",
    "        print(f\"T·ªïng s·ªë node ban ƒë·∫ßu: {total_nodes}\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PH√ÇN C·ª§M L·∫¶N ƒê·∫¶U TI√äN\")\n",
    "        print(f\"{'='*60}\")\n",
    "        clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "\n",
    "        initial_clusters_data_for_metrics = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_nodes = np.array([node_positions[nid] for nid in cinfo['nodes']])\n",
    "            initial_clusters_data_for_metrics.append((cluster_nodes, cinfo['nodes']))\n",
    "        initial_metrics = clustering.calculate_metrics(initial_clusters_data_for_metrics) if clusters else {'num_clusters':0,'avg_cluster_size':0,'min_cluster_size':0,'max_cluster_size':0,'avg_intra_distance':0,'max_intra_distance':0,'balance_score':0}\n",
    "\n",
    "        centers = [(200, 200, 400)] + [tuple(clusters[k]['center']) for k in sorted(clusters.keys())]\n",
    "        print(f\"\\nüîç T√≠nh ƒë∆∞·ªùng ƒëi tham lam ban ƒë·∫ßu (nearest-time)...\")\n",
    "        seed_path = nearest_neighbor_path(centers, v_f, v_AUV)\n",
    "        seed_time = travel_time(seed_path, centers, v_f, v_AUV)\n",
    "        seed_distance = path_distance(seed_path, centers)\n",
    "        # Greedy kh√¥ng c·∫£i thi·ªán sau seed; best = seed\n",
    "        best_path = seed_path[:]\n",
    "        best_time = seed_time\n",
    "        best_distance = seed_distance\n",
    "        print(f\"‚úÖ Seed time={seed_time:.2f}s | Dist={seed_distance:.2f}m | Clusters={len(clusters)}\")\n",
    "\n",
    "        cycle = 0\n",
    "        alive_log = []\n",
    "        energy_log = []\n",
    "        cluster_count_log = []\n",
    "        reclustering_cycles = []\n",
    "        reclustering_details = []\n",
    "        death_events = []\n",
    "        reoptimization_count = 0\n",
    "        stop_reason = \"\"\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üöÄ B·∫ÆT ƒê·∫¶U M√î PH·ªéNG (Greedy)\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        while True:\n",
    "            cycle += 1\n",
    "            alive_log.append(len(all_nodes))\n",
    "            total_energy = sum(all_nodes[n]['residual_energy'] for n in all_nodes)\n",
    "            energy_log.append(total_energy)\n",
    "            cluster_count_log.append(len(clusters))\n",
    "\n",
    "            alive_ratio = len(all_nodes)/total_nodes if total_nodes > 0 else 0\n",
    "            if alive_ratio < 0.9:\n",
    "                stop_reason = f\"Alive ratio below 90% ({alive_ratio*100:.2f}%)\"\n",
    "                print(f\"\\nüõë D·ª´ng ·ªü cycle {cycle}: {stop_reason}\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Cycle {cycle} --- | Alive: {alive_ratio*100:.2f}% ({len(all_nodes)}/{total_nodes}) | Energy: {total_energy:.2f}J | Clusters: {len(clusters)}\")\n",
    "            clusters = reselect_cluster_heads(clusters, all_nodes)\n",
    "\n",
    "            # C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng v·ªõi best_time hi·ªán t·∫°i\n",
    "            update_energy(all_nodes, clusters, best_time)\n",
    "\n",
    "            clusters, dead_nodes = remove_dead_nodes(all_nodes, clusters)\n",
    "            if dead_nodes:\n",
    "                print(f\"   ‚ö° {len(dead_nodes)} node(s) ch·∫øt: {dead_nodes}\")\n",
    "                death_events.append({'cycle': cycle,'dead_count': len(dead_nodes),'dead_ids': dead_nodes})\n",
    "                if len(all_nodes) == 0:\n",
    "                    stop_reason = \"All nodes dead\"\n",
    "                    print(\"   ‚ö†Ô∏è Kh√¥ng c√≤n node s·ªëng\")\n",
    "                    break\n",
    "                print(\"   üîß Ph√¢n c·ª•m l·∫°i do node ch·∫øt...\")\n",
    "                clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "                if len(clusters) == 0:\n",
    "                    stop_reason = \"No clusters after reclustering\"\n",
    "                    print(\"   ‚ö†Ô∏è Kh√¥ng c√≤n c·ª•m h·ª£p l·ªá\")\n",
    "                    break\n",
    "                reclustering_cycles.append(cycle)\n",
    "                reclustering_details.append({'cycle': cycle,'cluster_count': len(clusters)})\n",
    "                centers = [(200, 200, 400)] + [tuple(clusters[k]['center']) for k in sorted(clusters.keys())]\n",
    "                seed_path_new = nearest_neighbor_path(centers, v_f, v_AUV)\n",
    "                seed_time_new = travel_time(seed_path_new, centers, v_f, v_AUV)\n",
    "                seed_distance_new = path_distance(seed_path_new, centers)\n",
    "                # Greedy: best == seed each reoptimization\n",
    "                best_path = seed_path_new[:]\n",
    "                best_time = seed_time_new\n",
    "                best_distance = seed_distance_new\n",
    "                reoptimization_count += 1\n",
    "                print(f\"   ‚úÖ New greedy path time={best_time:.2f}s dist={best_distance:.2f}m clusters={len(clusters)}\")\n",
    "\n",
    "        if not stop_reason:\n",
    "            stop_reason = \"Loop ended\"\n",
    "\n",
    "        total_dead_nodes = total_nodes - len(all_nodes)\n",
    "\n",
    "        final_clusters_data_for_metrics = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_nodes = np.array([node_positions[nid] for nid in cinfo['nodes']])\n",
    "            final_clusters_data_for_metrics.append((cluster_nodes, cinfo['nodes']))\n",
    "        final_metrics = clustering.calculate_metrics(final_clusters_data_for_metrics) if clusters else {'num_clusters':0,'avg_cluster_size':0,'min_cluster_size':0,'max_cluster_size':0,'avg_intra_distance':0,'max_intra_distance':0,'balance_score':0}\n",
    "\n",
    "        final_total_energy = sum(nd['residual_energy'] for nd in all_nodes.values())\n",
    "        avg_energy_alive_final = final_total_energy/len(all_nodes) if all_nodes else 0.0\n",
    "        per_cluster_energy_final = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_energy = sum(all_nodes[nid]['residual_energy'] for nid in cinfo['nodes'] if nid in all_nodes)\n",
    "            per_cluster_energy_final.append({'cluster_id': cid,'nodes': cinfo['nodes'],'cluster_head': cinfo['cluster_head'],'total_energy': cluster_energy})\n",
    "\n",
    "        dead_node_ids = sorted({nid for ev in death_events for nid in ev['dead_ids']})\n",
    "        death_cycle_counts = {ev['cycle']: ev['dead_count'] for ev in death_events}\n",
    "        first_death_cycle = death_events[0]['cycle'] if death_events else None\n",
    "        last_death_cycle = death_events[-1]['cycle'] if death_events else None\n",
    "        mean_deaths_per_event = (total_dead_nodes/len(death_events)) if death_events else 0.0\n",
    "        cycles_until_first_death = first_death_cycle if first_death_cycle is not None else None\n",
    "\n",
    "        meta = {\n",
    "            'input_file': filename,\n",
    "            'method': 'Time-based Greedy',\n",
    "            'strategy': 'Reselect CH each cycle; Recluster+Greedy on node death',\n",
    "            'parameters': {\n",
    "                'r_sen': R_SEN,\n",
    "                'max_cluster_size': MAX_SIZE,\n",
    "                'min_cluster_size': MIN_SIZE,\n",
    "                'v_flow': v_f,\n",
    "                'v_AUV': v_AUV\n",
    "            },\n",
    "            'lifecycle': {\n",
    "                'cycles_completed': cycle-1,\n",
    "                'stop_reason': stop_reason,\n",
    "                'reoptimization_count': reoptimization_count,\n",
    "                'reclustering_cycles': reclustering_cycles,\n",
    "                'reclustering_details': reclustering_details\n",
    "            },\n",
    "            'nodes_summary': {\n",
    "                'initial_total_nodes': total_nodes,\n",
    "                'final_alive_nodes': len(all_nodes),\n",
    "                'final_alive_ratio': len(all_nodes)/total_nodes if total_nodes>0 else 0,\n",
    "                'total_dead_nodes': total_dead_nodes,\n",
    "                'dead_node_ids': dead_node_ids\n",
    "            },\n",
    "            'path_stats': {\n",
    "                'final_best_time': best_time,\n",
    "                'final_best_distance': best_distance,\n",
    "                'initial_seed_time': seed_time,\n",
    "                'initial_best_time': seed_time,\n",
    "                'initial_improvement_ratio': 0.0,\n",
    "                'final_improvement_ratio': 0.0\n",
    "            },\n",
    "            'clustering_initial': initial_metrics,\n",
    "            'clustering_final': final_metrics,\n",
    "            'energy': {\n",
    "                'initial_total_energy': initial_total_energy,\n",
    "                'final_total_energy': final_total_energy,\n",
    "                'avg_energy_alive_final': avg_energy_alive_final,\n",
    "                'per_cluster_energy_final': per_cluster_energy_final\n",
    "            },\n",
    "            'death_events': death_events,\n",
    "            'death_analytics': {\n",
    "                'death_cycle_counts': death_cycle_counts,\n",
    "                'first_death_cycle': first_death_cycle,\n",
    "                'last_death_cycle': last_death_cycle,\n",
    "                'mean_deaths_per_event': mean_deaths_per_event,\n",
    "                'cycles_until_first_death': cycles_until_first_death\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_json = os.path.join(output_folder, f\"result_{filename}\")\n",
    "        with open(output_json, 'w') as f:\n",
    "            json.dump(meta, f, indent=4)\n",
    "\n",
    "        results_summary.append((filename, cycle-1, len(reclustering_cycles), total_dead_nodes))\n",
    "        print(f\"\\n‚úÖ {filename}: cycles={cycle-1}, recluster={len(reclustering_cycles)}, dead={total_dead_nodes}\")\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.plot(range(len(alive_log)), alive_log, marker='o', linewidth=2, color='steelblue')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Alive nodes - {filename}\")\n",
    "        plt.xlabel(\"Cycle\"); plt.ylabel(\"Alive\"); plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=total_nodes*0.9, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(range(len(energy_log)), energy_log, marker='s', linewidth=2, color='orange')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Total energy - {filename}\"); plt.xlabel(\"Cycle\"); plt.ylabel(\"Energy (J)\"); plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.plot(range(1, len(cluster_count_log)+1), cluster_count_log, marker='^', linewidth=2, color='green')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Clusters per cycle - {filename}\"); plt.xlabel(\"Cycle\"); plt.ylabel(\"Clusters\"); plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.axis('off')\n",
    "        alive_ratio_final = len(all_nodes)/total_nodes if total_nodes>0 else 0\n",
    "        death_lines = []\n",
    "        for ev in death_events[:6]: death_lines.append(f\"  ‚Ä¢ C{ev['cycle']}: -{ev['dead_count']}\")\n",
    "        if len(death_events) > 6: death_lines.append(\"  ‚Ä¢ ...\")\n",
    "        death_block = \"\\n\".join(death_lines) if death_lines else \"  ‚Ä¢ (None)\"\n",
    "        info_text = f\"\"\"SUMMARY\\nCycles: {cycle-1}\\nRecluster events: {len(reclustering_cycles)}\\nTotal dead: {total_dead_nodes}\\nAlive: {len(all_nodes)}/{total_nodes} ({alive_ratio_final*100:.1f}%)\\nStop: {stop_reason}\\nDeaths:\\n{death_block}\"\"\"\n",
    "        plt.text(0.02, 0.05, info_text, fontsize=9, family='monospace', va='bottom')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f\"summary_{filename}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    if results_summary:\n",
    "        print(f\"\\n{'='*60}\\nüìä T·ªîNG K·∫æT\\n{'='*60}\")\n",
    "        for fname, cycles, recount, dead in results_summary:\n",
    "            print(f\"  {fname}: cycles={cycles}, recluster={recount}, dead={dead}\")\n",
    "        print(f\"\\n‚úÖ Ho√†n th√†nh! K·∫øt qu·∫£ t·∫°i: {output_folder}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra\")\n",
    "\n",
    "print(\"‚úì Main function (greedy enriched) loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec634f8-0d7a-403b-80bf-cfdfe71bad34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.247359Z",
     "iopub.status.busy": "2025-11-23T18:27:14.247083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8816109,
     "sourceId": 13841806,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
