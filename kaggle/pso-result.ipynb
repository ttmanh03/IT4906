{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5c394",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.078079Z",
     "iopub.status.busy": "2025-11-23T18:27:14.077669Z",
     "iopub.status.idle": "2025-11-23T18:27:14.083183Z",
     "shell.execute_reply": "2025-11-23T18:27:14.082091Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.078054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aecbdac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.084807Z",
     "iopub.status.busy": "2025-11-23T18:27:14.084505Z",
     "iopub.status.idle": "2025-11-23T18:27:14.111429Z",
     "shell.execute_reply": "2025-11-23T18:27:14.110364Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.084778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Clustering:\n",
    "    def __init__(self, space_size=400, r_sen=50, max_cluster_size=20, min_cluster_size=5):\n",
    "        self.space_size = space_size\n",
    "        self.r_sen = r_sen\n",
    "        self.max_cluster_size = max_cluster_size\n",
    "        self.min_cluster_size = min_cluster_size\n",
    "\n",
    "    def estimate_optimal_k(self, nodes, base_station=(200,200,400)):\n",
    "        \"\"\"\n",
    "        ∆Ø·ªõc t√≠nh s·ªë c·ª•m t·ªëi ∆∞u d·ª±a tr√™n c√¥ng th·ª©c WSN\n",
    "        K = sqrt(N*L / (pi*d_tobs))\n",
    "        \"\"\"\n",
    "        N = len(nodes)\n",
    "        base_pos = np.array(base_station)\n",
    "\n",
    "        # Khoang cach trung binh toi base station\n",
    "        distances = np.linalg.norm(nodes - base_pos, axis=1)\n",
    "        d_tobs = np.mean(distances)\n",
    "\n",
    "        space_size = self.space_size\n",
    "\n",
    "        k_optimal = np.sqrt(N * space_size / (np.pi * d_tobs))\n",
    "        k_optimal = max(2, int(np.round(k_optimal)))\n",
    "\n",
    "        # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n max_cluster_size\n",
    "        k_min = int(np.ceil(N / self.max_cluster_size))\n",
    "        k_optimal = max(k_optimal, k_min)\n",
    "        \n",
    "        return k_optimal\n",
    "    \n",
    "    def check_cluster_validity(self, cluster_nodes):\n",
    "        \"\"\"\n",
    "        Kiem tra tinh hop le cua cum\n",
    "        \"\"\"\n",
    "        size = len(cluster_nodes)\n",
    "\n",
    "        # Ki·ªÉm tra k√≠ch th∆∞·ªõc\n",
    "        if size < self.min_cluster_size or size > self.max_cluster_size:\n",
    "            return False, 0, size\n",
    "        \n",
    "        # Ki·ªÉm tra kho·∫£ng c√°ch\n",
    "        if size > 1:\n",
    "            distances = pdist(cluster_nodes)\n",
    "            max_dist = np.max(distances)\n",
    "            \n",
    "            if max_dist > self.r_sen:\n",
    "                return False, max_dist, size\n",
    "            \n",
    "            return True, max_dist, size\n",
    "        \n",
    "        return True, 0, size\n",
    "    \n",
    "    def split_invalid_cluster(self, cluster_nodes, cluster_ids):\n",
    "        \"\"\"\n",
    "        Chia nh·ªè c·ª•m kh√¥ng h·ª£p l·ªá th√†nh c√°c c·ª•m con\n",
    "        \"\"\"\n",
    "        # N·∫øu c·ª•m ch·ªâ c√≥ 1 node, kh√¥ng th·ªÉ chia\n",
    "        if len(cluster_nodes) < 2:\n",
    "            return [(cluster_nodes, cluster_ids)]\n",
    "        \n",
    "        # S·ª≠ d·ª•ng K-Means ƒë·ªÉ chia 2\n",
    "        kmeans = KMeans(n_clusters=2, n_init=20, random_state=42)\n",
    "        labels = kmeans.fit_predict(cluster_nodes)\n",
    "        \n",
    "        sub_clusters = []\n",
    "        for i in range(2):\n",
    "            sub_nodes = cluster_nodes[labels == i]\n",
    "            sub_ids = [cluster_ids[j] for j in range(len(cluster_ids)) if labels[j] == i]\n",
    "            \n",
    "            if len(sub_nodes) > 0:\n",
    "                sub_clusters.append((sub_nodes, sub_ids))\n",
    "        \n",
    "        return sub_clusters\n",
    "    \n",
    "    def merge_small_clusters(self, clusters_data):\n",
    "        \"\"\"\n",
    "        G·ªôp c√°c c·ª•m nh·ªè v·ªõi c·ª•m l√°ng gi·ªÅng g·∫ßn nh·∫•t\n",
    "        \"\"\"\n",
    "        if len(clusters_data) <= 1:\n",
    "            return clusters_data\n",
    "        \n",
    "        merged = []\n",
    "        to_merge = []\n",
    "        \n",
    "        # T√¨m c√°c c·ª•m nh·ªè\n",
    "        for nodes, ids in clusters_data:\n",
    "            if len(nodes) < self.min_cluster_size:\n",
    "                to_merge.append((nodes, ids))\n",
    "            else:\n",
    "                merged.append((nodes, ids))\n",
    "        \n",
    "        # G·ªôp t·ª´ng c·ª•m nh·ªè v√†o c·ª•m g·∫ßn nh·∫•t\n",
    "        for small_nodes, small_ids in to_merge:\n",
    "            if len(merged) == 0:\n",
    "                merged.append((small_nodes, small_ids))\n",
    "                continue\n",
    "            \n",
    "            # T√¨m c·ª•m g·∫ßn nh·∫•t\n",
    "            small_center = np.mean(small_nodes, axis=0)\n",
    "            min_dist = float('inf')\n",
    "            best_idx = 0\n",
    "            \n",
    "            for i, (nodes, ids) in enumerate(merged):\n",
    "                center = np.mean(nodes, axis=0)\n",
    "                dist = np.linalg.norm(small_center - center)\n",
    "                \n",
    "                # Ki·ªÉm tra xem g·ªôp c√≥ v∆∞·ª£t qu√° max_size kh√¥ng\n",
    "                if dist < min_dist and len(nodes) + len(small_nodes) <= self.max_cluster_size:\n",
    "                    min_dist = dist\n",
    "                    best_idx = i\n",
    "            \n",
    "            # G·ªôp\n",
    "            merged[best_idx] = (\n",
    "                np.vstack([merged[best_idx][0], small_nodes]),\n",
    "                merged[best_idx][1] + small_ids\n",
    "            )\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    def cluster_with_constraints(self, nodes, node_ids, k=None, max_iterations=10):\n",
    "        \"\"\"\n",
    "        Ph√¢n c·ª•m v·ªõi r√†ng bu·ªôc - Thu·∫≠t to√°n ch√≠nh\n",
    "        \n",
    "        Args:\n",
    "            nodes: T·ªça ƒë·ªô 3D c·ªßa nodes\n",
    "            node_ids: ID c·ªßa nodes\n",
    "            k: S·ªë c·ª•m (n·∫øu None s·∫Ω t·ª± ƒë·ªông ∆∞·ªõc t√≠nh)\n",
    "            max_iterations: S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa ƒë·ªÉ ƒëi·ªÅu ch·ªânh\n",
    "            \n",
    "        Returns:\n",
    "            List of (cluster_nodes, cluster_ids)\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.estimate_optimal_k(nodes)\n",
    "        \n",
    "        print(f\"B·∫Øt ƒë·∫ßu ph√¢n c·ª•m v·ªõi k={k}\")\n",
    "        \n",
    "        # B∆∞·ªõc 1: K-Means ban ƒë·∫ßu\n",
    "        kmeans = KMeans(n_clusters=k, n_init=30, random_state=42)\n",
    "        labels = kmeans.fit_predict(nodes)\n",
    "        \n",
    "        # B∆∞·ªõc 2: T·∫°o c√°c c·ª•m v√† ki·ªÉm tra\n",
    "        iteration = 0\n",
    "        while iteration < max_iterations:\n",
    "            print(f\"  V√≤ng l·∫∑p {iteration + 1}/{max_iterations}\")\n",
    "            \n",
    "            valid_clusters = []\n",
    "            invalid_clusters = []\n",
    "            \n",
    "            # Ph√¢n lo·∫°i c·ª•m h·ª£p l·ªá v√† kh√¥ng h·ª£p l·ªá\n",
    "            for i in range(k):\n",
    "                cluster_nodes = nodes[labels == i]\n",
    "                cluster_ids = [node_ids[j] for j in range(len(node_ids)) if labels[j] == i]\n",
    "                \n",
    "                if len(cluster_nodes) == 0:\n",
    "                    continue\n",
    "                \n",
    "                is_valid, max_dist, size = self.check_cluster_validity(cluster_nodes)\n",
    "                \n",
    "                if is_valid:\n",
    "                    valid_clusters.append((cluster_nodes, cluster_ids))\n",
    "                    print(f\"    C·ª•m {i}: ‚úì h·ª£p l·ªá (size={size}, max_dist={max_dist:.1f}m)\")\n",
    "                else:\n",
    "                    invalid_clusters.append((cluster_nodes, cluster_ids))\n",
    "                    print(f\"    C·ª•m {i}: ‚úó kh√¥ng h·ª£p l·ªá (size={size}, max_dist={max_dist:.1f}m)\")\n",
    "            \n",
    "            # N·∫øu t·∫•t c·∫£ h·ª£p l·ªá, k·∫øt th√∫c\n",
    "            if len(invalid_clusters) == 0:\n",
    "                print(f\"  ‚Üí T·∫•t c·∫£ c·ª•m h·ª£p l·ªá!\")\n",
    "                break\n",
    "            \n",
    "            # B∆∞·ªõc 3: X·ª≠ l√Ω c√°c c·ª•m kh√¥ng h·ª£p l·ªá\n",
    "            for cluster_nodes, cluster_ids in invalid_clusters:\n",
    "                size = len(cluster_nodes)\n",
    "                \n",
    "                if size > self.max_cluster_size:\n",
    "                    # C·ª•m qu√° l·ªõn ‚Üí Chia nh·ªè\n",
    "                    print(f\"    ‚Üí Chia c·ª•m (size={size})\")\n",
    "                    sub_clusters = self.split_invalid_cluster(cluster_nodes, cluster_ids)\n",
    "                    valid_clusters.extend(sub_clusters)\n",
    "                else:\n",
    "                    # C·ª•m c√≥ kho·∫£ng c√°ch qu√° l·ªõn ‚Üí Chia nh·ªè\n",
    "                    print(f\"    ‚Üí Chia c·ª•m (kho·∫£ng c√°ch l·ªõn)\")\n",
    "                    sub_clusters = self.split_invalid_cluster(cluster_nodes, cluster_ids)\n",
    "                    valid_clusters.extend(sub_clusters)\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t labels v√† k cho v√≤ng l·∫∑p ti·∫øp theo\n",
    "            k = len(valid_clusters)\n",
    "            \n",
    "            # T·∫°o l·∫°i labels t·ª´ valid_clusters\n",
    "            labels = np.zeros(len(nodes), dtype=int)\n",
    "            for cluster_idx, (_, cluster_ids) in enumerate(valid_clusters):\n",
    "                for node_id in cluster_ids:\n",
    "                    node_idx = node_ids.index(node_id)\n",
    "                    labels[node_idx] = cluster_idx\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # B∆∞·ªõc 4: G·ªôp c√°c c·ª•m qu√° nh·ªè\n",
    "        valid_clusters = self.merge_small_clusters(valid_clusters)\n",
    "        \n",
    "        print(f\"Ho√†n th√†nh: {len(valid_clusters)} c·ª•m\")\n",
    "        return valid_clusters\n",
    "    \n",
    "    def choose_cluster_head(self, cluster_nodes, cluster_ids, node_data=None):\n",
    "        \"\"\"\n",
    "        Ch·ªçn cluster head\n",
    "        - ∆Øu ti√™n: Node c√≥ nƒÉng l∆∞·ª£ng cao nh·∫•t\n",
    "        - D·ª± ph√≤ng: Node g·∫ßn t√¢m c·ª•m nh·∫•t\n",
    "        \"\"\"\n",
    "        if node_data:\n",
    "            # Ch·ªçn theo nƒÉng l∆∞·ª£ng\n",
    "            max_energy = -1\n",
    "            ch_id = cluster_ids[0]\n",
    "            \n",
    "            for nid in cluster_ids:\n",
    "                if nid in node_data and 'residual_energy' in node_data[nid]:\n",
    "                    energy = node_data[nid]['residual_energy']\n",
    "                    if energy > max_energy:\n",
    "                        max_energy = energy\n",
    "                        ch_id = nid\n",
    "            \n",
    "            return ch_id\n",
    "        else:\n",
    "            # Ch·ªçn theo kho·∫£ng c√°ch ƒë·∫øn t√¢m\n",
    "            center = np.mean(cluster_nodes, axis=0)\n",
    "            distances = np.linalg.norm(cluster_nodes - center, axis=1)\n",
    "            min_idx = np.argmin(distances)\n",
    "            return cluster_ids[min_idx]\n",
    "    \n",
    "    def calculate_metrics(self, clusters_data):\n",
    "        \"\"\"\n",
    "        T√≠nh c√°c metric ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m\n",
    "        \"\"\"\n",
    "        metrics = {\n",
    "            'num_clusters': len(clusters_data),\n",
    "            'avg_cluster_size': 0,\n",
    "            'min_cluster_size': float('inf'),\n",
    "            'max_cluster_size': 0,\n",
    "            'avg_intra_distance': 0,\n",
    "            'max_intra_distance': 0,\n",
    "            'balance_score': 0  # ƒê·ªô c√¢n b·∫±ng k√≠ch th∆∞·ªõc c·ª•m (0-1, c√†ng cao c√†ng t·ªët)\n",
    "        }\n",
    "        \n",
    "        sizes = []\n",
    "        intra_dists = []\n",
    "        \n",
    "        for nodes, ids in clusters_data:\n",
    "            size = len(nodes)\n",
    "            sizes.append(size)\n",
    "            \n",
    "            metrics['min_cluster_size'] = min(metrics['min_cluster_size'], size)\n",
    "            metrics['max_cluster_size'] = max(metrics['max_cluster_size'], size)\n",
    "            \n",
    "            if size > 1:\n",
    "                distances = pdist(nodes)\n",
    "                intra_dists.append(np.mean(distances))\n",
    "                metrics['max_intra_distance'] = max(metrics['max_intra_distance'], np.max(distances))\n",
    "        \n",
    "        metrics['avg_cluster_size'] = np.mean(sizes)\n",
    "        metrics['avg_intra_distance'] = np.mean(intra_dists) if intra_dists else 0\n",
    "        \n",
    "        # T√≠nh balance score (d·ª±a tr√™n coefficient of variation)\n",
    "        cv = np.std(sizes) / np.mean(sizes) if np.mean(sizes) > 0 else 0\n",
    "        metrics['balance_score'] = 1 / (1 + cv)  # 1 = ho√†n to√†n c√¢n b·∫±ng\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7ebeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.112497Z",
     "iopub.status.busy": "2025-11-23T18:27:14.112227Z",
     "iopub.status.idle": "2025-11-23T18:27:14.130428Z",
     "shell.execute_reply": "2025-11-23T18:27:14.129599Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.112478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1,y1,z1 = p1; x2,y2,z2 = p2\n",
    "    Lx,Ly,Lz = x2-x1,y2-y1,z2-z1\n",
    "    L_mag = math.sqrt(Lx**2+Ly**2+Lz**2)\n",
    "    if L_mag == 0: return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta)/v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    if abs(cos_beta) < 1e-9: return v_AUV\n",
    "    return abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    if len(path) <= 1: return 0.0\n",
    "    t = 0.0\n",
    "    for i in range(len(path)-1):\n",
    "        p1,p2 = coords[path[i]], coords[path[i+1]]\n",
    "        d = np.linalg.norm(np.array(p2)-np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        t += d / max(v_s, 1e-9)\n",
    "    p1,p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2)-np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    t += d / max(v_s, 1e-9)\n",
    "    return t\n",
    "\n",
    "def path_distance(path, coords):\n",
    "    if len(path) <= 1: return 0.0\n",
    "    dist = 0.0\n",
    "    for i in range(len(path)-1):\n",
    "        p1,p2 = coords[path[i]], coords[path[i+1]]\n",
    "        dist += np.linalg.norm(np.array(p2)-np.array(p1))\n",
    "    p1,p2 = coords[path[-1]], coords[path[0]]\n",
    "    dist += np.linalg.norm(np.array(p2)-np.array(p1))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333849b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.132310Z",
     "iopub.status.busy": "2025-11-23T18:27:14.132074Z",
     "iopub.status.idle": "2025-11-23T18:27:14.161116Z",
     "shell.execute_reply": "2025-11-23T18:27:14.159780Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.132294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_swap_sequence(A, B):\n",
    "    seq = []\n",
    "    temp = A.copy()\n",
    "    for i in range(1, len(A)): # B·∫Øt ƒë·∫ßu t·ª´ 1 v√¨ 0 l√† c·ªë ƒë·ªãnh\n",
    "        if temp[i] != B[i]:\n",
    "            # T√¨m v·ªã tr√≠ c·ªßa ph·∫ßn t·ª≠ B[i] trong temp\n",
    "            try:\n",
    "                j = temp.index(B[i])\n",
    "                seq.append((i, j))\n",
    "                temp[i], temp[j] = temp[j], temp[i]\n",
    "            except ValueError:\n",
    "                # X·∫£y ra khi A v√† B kh√¥ng c√≥ c√πng c√°c ph·∫ßn t·ª≠ (kh√¥ng n√™n x·∫£y ra)\n",
    "                pass \n",
    "    return seq\n",
    "\n",
    "def apply_velocity(position, velocity):\n",
    "    pos = position.copy()\n",
    "    for i, j in velocity:\n",
    "        if i > 0 and j > 0 and i < len(pos) and j < len(pos): # B·∫£o v·ªá index\n",
    "            pos[i], pos[j] = pos[j], pos[i]\n",
    "    return pos\n",
    "\n",
    "def pso_tsp_3d_time(coords, v_f=1.0, v_AUV=3.0,\n",
    "                    n_particles=40, max_iter=200,\n",
    "                    w=0.7, c1=0.5, c2=0.5, init_gbest=None, verbose=True):\n",
    "    \n",
    "    n_cities = len(coords)\n",
    "    if n_cities < 2: return [0], 0.0\n",
    "    if n_cities == 2: return [0, 1], travel_time([0, 1], coords, v_f, v_AUV)\n",
    "    \n",
    "    cities = list(range(1, n_cities))\n",
    "\n",
    "    # Kh·ªüi t·∫°o qu·∫ßn th·ªÉ\n",
    "    swarm = [[0] + random.sample(cities, len(cities)) for _ in range(n_particles)]\n",
    "    velocities = [[] for _ in range(n_particles)]\n",
    "\n",
    "    # ƒê√°nh gi√° ban ƒë·∫ßu\n",
    "    costs = [travel_time(p, coords, v_f, v_AUV) for p in swarm]\n",
    "    pbest = [p.copy() for p in swarm]\n",
    "    pbest_cost = costs.copy()\n",
    "\n",
    "    # X√°c ƒë·ªãnh gbest\n",
    "    if init_gbest is not None and len(init_gbest) == n_cities:\n",
    "        gbest = init_gbest.copy()\n",
    "        gbest_cost = travel_time(gbest, coords, v_f, v_AUV)\n",
    "    else:\n",
    "        best_idx = int(np.argmin(pbest_cost))\n",
    "        gbest = pbest[best_idx].copy()\n",
    "        gbest_cost = pbest_cost[best_idx]\n",
    "\n",
    "    # --- V√≤ng l·∫∑p ch√≠nh ---\n",
    "    for t in range(max_iter):\n",
    "        inertia = 0.7 - 0.5 * (t / max_iter) # Gi·∫£m d·∫ßn inertia\n",
    "\n",
    "        for i in range(n_particles):\n",
    "            xi, vi = swarm[i], velocities[i]\n",
    "\n",
    "            # Gi·ªØ l·∫°i 1 ph·∫ßn v·∫≠n t·ªëc c≈©\n",
    "            v_new = vi[:int(inertia * len(vi))]\n",
    "\n",
    "            # ·∫¢nh h∆∞·ªüng c√° nh√¢n (pbest)\n",
    "            if random.random() < c1:\n",
    "                seq_pb = get_swap_sequence(xi, pbest[i])\n",
    "                if seq_pb:\n",
    "                    v_new += random.sample(seq_pb, min(2, len(seq_pb))) # Th√™m 2 swap ng·∫´u nhi√™n\n",
    "\n",
    "            # ·∫¢nh h∆∞·ªüng to√†n c·ª•c (gbest)\n",
    "            if random.random() < c2:\n",
    "                seq_gb = get_swap_sequence(xi, gbest)\n",
    "                if seq_gb:\n",
    "                    v_new += random.sample(seq_gb, min(2, len(seq_gb))) # Th√™m 2 swap ng·∫´u nhi√™n\n",
    "\n",
    "            # C·∫≠p nh·∫≠t v·ªã tr√≠ v√† v·∫≠n t·ªëc\n",
    "            new_x = apply_velocity(xi, v_new)\n",
    "            new_cost = travel_time(new_x, coords, v_f, v_AUV)\n",
    "            \n",
    "            swarm[i], velocities[i] = new_x, v_new\n",
    "\n",
    "            # C·∫≠p nh·∫≠t pbest v√† gbest\n",
    "            if new_cost < pbest_cost[i]:\n",
    "                pbest[i], pbest_cost[i] = new_x, new_cost\n",
    "                if new_cost < gbest_cost:\n",
    "                    gbest, gbest_cost = new_x, new_cost\n",
    "\n",
    "        if verbose and t % 50 == 0:\n",
    "            print(f\"    [PSO Iter {t:3d}]: Best time = {gbest_cost:.4f}\")\n",
    "\n",
    "    if verbose:\n",
    "         print(f\"    [PSO Iter {max_iter:3d}]: Final Best time = {gbest_cost:.4f}\")\n",
    "    return gbest, gbest_cost\n",
    "\n",
    "def multi_pso_tsp(coords, v_f=1.2, v_AUV=3.0, n_outer=5, verbose=True, **kwargs):\n",
    "    \"\"\"Ch·∫°y PSO nhi·ªÅu l·∫ßn ƒë·ªÉ c·∫£i thi·ªán h·ªôi t·ª• (T·ª´ File 4)\"\"\"\n",
    "    prev_gbest, prev_cost = None, float('inf')\n",
    "\n",
    "    for outer in range(1, n_outer + 1):\n",
    "        if verbose:\n",
    "            print(f\"   [PSO Outer loop {outer}/{n_outer}]\")\n",
    "            \n",
    "        gbest, cost = pso_tsp_3d_time(coords, v_f=v_f, v_AUV=v_AUV,\n",
    "                                      init_gbest=prev_gbest, verbose=verbose, **kwargs)\n",
    "        \n",
    "        if cost < prev_cost:\n",
    "            prev_gbest, prev_cost = gbest, cost\n",
    "        \n",
    "        # C√≥ th·ªÉ th√™m ƒëi·ªÅu ki·ªán d·ª´ng s·ªõm n·∫øu kh√¥ng c·∫£i thi·ªán\n",
    "        # if abs(cost - prev_cost) < 1e-6:\n",
    "        #     print(\"   ‚Üí Converged! Stop early.\")\n",
    "        #     break\n",
    "\n",
    "    return prev_gbest, prev_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e739a74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.162724Z",
     "iopub.status.busy": "2025-11-23T18:27:14.162201Z",
     "iopub.status.idle": "2025-11-23T18:27:14.179942Z",
     "shell.execute_reply": "2025-11-23T18:27:14.178976Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.162697Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_energy(best_time, n_members):\n",
    "    \"\"\"\n",
    "    T√≠nh nƒÉng l∆∞·ª£ng ti√™u th·ª• cho Member Node v√† Cluster Head.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    - n_members: S·ªë l∆∞·ª£ng node th√†nh vi√™n th·ª±c t·∫ø trong cluster (kh√¥ng t√≠nh cluster head)\n",
    "    \"\"\"\n",
    "    G, L = 100, 1024\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Member Node\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Cluster Head (nh·∫≠n t·ª´ n_members node, truy·ªÅn cho AUV)\n",
    "    E_rx_TN = G * P_r * L * n_members / DR\n",
    "    E_tx_TN = G * P_t * L * n_members / DR_i\n",
    "    E_idle_TN = (best_time - (G*L*n_members/DR) - (G*L*n_members/DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "\n",
    "    return {\n",
    "        \"Member\": {\"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "def update_energy(all_nodes, clusters, best_time):\n",
    "    \"\"\"\n",
    "    C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng cho t·∫•t c·∫£ c√°c node d·ª±a tr√™n s·ªë member th·ª±c t·∫ø c·ªßa t·ª´ng cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary ch·ª©a th√¥ng tin t·∫•t c·∫£ c√°c node\n",
    "    - clusters: Dictionary ch·ª©a th√¥ng tin c√°c cluster\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    \"\"\"\n",
    "    for cid, cinfo in clusters.items():\n",
    "        ch = cinfo.get('cluster_head')\n",
    "        nodes = cinfo.get('nodes', [])\n",
    "        \n",
    "        # T√≠nh s·ªë member nodes (kh√¥ng t√≠nh cluster head)\n",
    "        n_members = len([n for n in nodes if n != ch])\n",
    "        \n",
    "        # T√≠nh nƒÉng l∆∞·ª£ng cho cluster n√†y v·ªõi s·ªë member th·ª±c t·∫ø\n",
    "        energy_report = compute_energy(best_time, n_members)\n",
    "        \n",
    "        for nid in nodes:\n",
    "            if nid not in all_nodes: continue\n",
    "            if nid == ch:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Target']['E_total']\n",
    "            else:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Member']['E_total']\n",
    "            all_nodes[nid]['residual_energy'] = max(all_nodes[nid]['residual_energy'], 0.0)\n",
    "\n",
    "def remove_dead_nodes(all_nodes, clusters):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè c√°c node ƒë√£ h·∫øt nƒÉng l∆∞·ª£ng v√† c·∫≠p nh·∫≠t l·∫°i clusters.\n",
    "    \n",
    "    Returns:\n",
    "    - new_clusters: Dictionary c√°c cluster c√≤n node s·ªëng\n",
    "    - dead: List c√°c node_id ƒë√£ ch·∫øt\n",
    "    \"\"\"\n",
    "    dead = [nid for nid, info in list(all_nodes.items()) if info['residual_energy'] <= 0]\n",
    "    for nid in dead:\n",
    "        del all_nodes[nid]\n",
    "\n",
    "    new_clusters = {}\n",
    "    for cid, cinfo in clusters.items():\n",
    "        alive_nodes = [nid for nid in cinfo.get('nodes', []) if nid in all_nodes]\n",
    "        if alive_nodes:\n",
    "            new_c = dict(cinfo)\n",
    "            new_c['nodes'] = alive_nodes\n",
    "            new_clusters[cid] = new_c\n",
    "\n",
    "    return new_clusters, dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4012ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.181784Z",
     "iopub.status.busy": "2025-11-23T18:27:14.181126Z",
     "iopub.status.idle": "2025-11-23T18:27:14.198877Z",
     "shell.execute_reply": "2025-11-23T18:27:14.197823Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.181740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def recluster(all_nodes, node_positions, clustering_instance, r_sen=60, max_size=20, min_size=5):\n",
    "    \"\"\"\n",
    "    Ph√¢n c·ª•m l·∫°i to√†n b·ªô c√°c node c√≤n s·ªëng s·ª≠ d·ª•ng thu·∫≠t to√°n t·ª´ cluster.py.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary c√°c node c√≤n s·ªëng\n",
    "    - node_positions: Dictionary v·ªã tr√≠ c·ªßa c√°c node\n",
    "    - clustering_instance: Instance c·ªßa class Clustering\n",
    "    - r_sen: Ng∆∞·ª°ng kho·∫£ng c√°ch t·ªëi ƒëa trong c·ª•m\n",
    "    - max_size: S·ªë l∆∞·ª£ng node t·ªëi ƒëa trong 1 c·ª•m\n",
    "    - min_size: S·ªë l∆∞·ª£ng node t·ªëi thi·ªÉu trong 1 c·ª•m\n",
    "    \n",
    "    Returns:\n",
    "    - clusters: Dictionary c√°c c·ª•m m·ªõi\n",
    "    \"\"\"\n",
    "    ids = sorted(list(all_nodes.keys()))\n",
    "    if len(ids) == 0:\n",
    "        return {}\n",
    "    coords = np.array([node_positions[nid] for nid in ids])\n",
    "    clustering_instance.r_sen = r_sen\n",
    "    clustering_instance.max_cluster_size = max_size\n",
    "    clustering_instance.min_cluster_size = min_size\n",
    "    clusters_data = clustering_instance.cluster_with_constraints(coords, ids)\n",
    "    clusters = {}\n",
    "    for i, (cluster_nodes, cluster_ids) in enumerate(clusters_data):\n",
    "        center = np.mean(cluster_nodes, axis=0).tolist()\n",
    "        ch = clustering_instance.choose_cluster_head(cluster_nodes, cluster_ids, all_nodes)\n",
    "        clusters[i] = {'nodes': cluster_ids, 'center': center, 'cluster_head': ch}\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6b93f0-bb87-4303-a031-ef3c71222f02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.200339Z",
     "iopub.status.busy": "2025-11-23T18:27:14.199982Z",
     "iopub.status.idle": "2025-11-23T18:27:14.217069Z",
     "shell.execute_reply": "2025-11-23T18:27:14.216159Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.200310Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reselect_cluster_heads(clusters, all_nodes):\n",
    "    \"\"\"\n",
    "    Ch·ªâ ch·ªçn l·∫°i cluster head cho c√°c c·ª•m hi·ªán t·∫°i d·ª±a tr√™n nƒÉng l∆∞·ª£ng.\n",
    "    Kh√¥ng ph√¢n c·ª•m l·∫°i.\n",
    "    \"\"\"\n",
    "    for cid, cinfo in clusters.items():\n",
    "        cluster_ids = cinfo['nodes']\n",
    "        # T√¨m node c√≥ nƒÉng l∆∞·ª£ng cao nh·∫•t\n",
    "        max_energy = -1\n",
    "        new_ch = cluster_ids[0]\n",
    "        for nid in cluster_ids:\n",
    "            if nid in all_nodes and 'residual_energy' in all_nodes[nid]:\n",
    "                energy = all_nodes[nid]['residual_energy']\n",
    "                if energy > max_energy:\n",
    "                    max_energy = energy\n",
    "                    new_ch = nid\n",
    "        clusters[cid]['cluster_head'] = new_ch\n",
    "    return clusters\n",
    "\n",
    "print(\"Helper function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1e08e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.218377Z",
     "iopub.status.busy": "2025-11-23T18:27:14.218067Z",
     "iopub.status.idle": "2025-11-23T18:27:14.246263Z",
     "shell.execute_reply": "2025-11-23T18:27:14.245286Z",
     "shell.execute_reply.started": "2025-11-23T18:27:14.218350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    M√¥ ph·ªèng PSO v·ªõi enriched metadata Option B:\n",
    "    - Reselect CH m·ªói cycle; Recluster + PSO khi node ch·∫øt.\n",
    "    - Ghi path stats (seed vs best), clustering metrics, energy summaries, death analytics.\n",
    "    \"\"\"\n",
    "    input_folder = \"/kaggle/input/nodes-data\"\n",
    "    output_folder = \"/kaggle/working/output_data_pso\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_folder):\n",
    "        print(f\"‚ùå L·ªói: Th∆∞ m·ª•c {input_folder} kh√¥ng t·ªìn t·∫°i!\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(input_folder) if f.endswith('.json')]\n",
    "    if len(files) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu n√†o trong {input_folder}\")\n",
    "        return\n",
    "\n",
    "    INITIAL_ENERGY = 100.0\n",
    "    v_f = 1.2\n",
    "    v_AUV = 3.0\n",
    "    R_SEN = 60\n",
    "    MAX_SIZE = 20\n",
    "    MIN_SIZE = 5\n",
    "\n",
    "    results_summary = []\n",
    "    clustering = Clustering(space_size=400, r_sen=R_SEN, max_cluster_size=MAX_SIZE, min_cluster_size=MIN_SIZE)\n",
    "\n",
    "    def seed_nearest_neighbor(coords):\n",
    "        n = len(coords)\n",
    "        if n <= 1: return [0]\n",
    "        unvisited = set(range(1,n))\n",
    "        path = [0]; cur = 0\n",
    "        while unvisited:\n",
    "            nxt = min(unvisited, key=lambda x: np.linalg.norm(np.array(coords[x]) - np.array(coords[cur])))\n",
    "            path.append(nxt); unvisited.remove(nxt); cur = nxt\n",
    "        return path\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\n{'='*60}\\n=== ƒêang x·ª≠ l√Ω file: {filename} (PSO) ===\\n{'='*60}\")\n",
    "        try:\n",
    "            with open(input_path,'r') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói ƒë·ªçc file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        node_positions = {}\n",
    "        all_nodes = {}\n",
    "        if isinstance(data, list):\n",
    "            for node in data:\n",
    "                nid = node['id']\n",
    "                all_nodes[nid] = {'initial_energy': node.get('initial_energy', INITIAL_ENERGY), 'residual_energy': node.get('residual_energy', INITIAL_ENERGY)}\n",
    "                node_positions[nid] = (node['x'], node['y'], node['z'])\n",
    "        else:\n",
    "            print(f\"‚ùå C·∫•u tr√∫c file {filename} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£\")\n",
    "            continue\n",
    "\n",
    "        total_nodes = len(all_nodes)\n",
    "        initial_total_energy = sum(nd['initial_energy'] for nd in all_nodes.values())\n",
    "        print(f\"T·ªïng s·ªë node ban ƒë·∫ßu: {total_nodes}\")\n",
    "\n",
    "        print(f\"\\n{'='*60}\\nPH√ÇN C·ª§M L·∫¶N ƒê·∫¶U TI√äN\\n{'='*60}\")\n",
    "        clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "\n",
    "        initial_clusters_data_for_metrics = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_nodes = np.array([node_positions[nid] for nid in cinfo['nodes']])\n",
    "            initial_clusters_data_for_metrics.append((cluster_nodes, cinfo['nodes']))\n",
    "        initial_metrics = clustering.calculate_metrics(initial_clusters_data_for_metrics) if clusters else {'num_clusters':0,'avg_cluster_size':0,'min_cluster_size':0,'max_cluster_size':0,'avg_intra_distance':0,'max_intra_distance':0,'balance_score':0}\n",
    "\n",
    "        sorted_keys = sorted(clusters.keys())\n",
    "        centers = [(200,200,400)] + [tuple(clusters[k]['center']) for k in sorted_keys]\n",
    "        center_coords = np.array(centers)\n",
    "\n",
    "        # Seed path (nearest neighbor) for baseline\n",
    "        seed_path = seed_nearest_neighbor(center_coords)\n",
    "        seed_time = travel_time(seed_path, center_coords, v_f, v_AUV)\n",
    "        seed_distance = path_distance(seed_path, center_coords)\n",
    "\n",
    "        print(f\"üåÄ Ch·∫°y PSO t·ªëi ∆∞u tour ban ƒë·∫ßu...\")\n",
    "        best_path, best_time = multi_pso_tsp(center_coords, v_f, v_AUV, n_outer=3, n_particles=30, max_iter=80, verbose=False)\n",
    "        best_distance = path_distance(best_path, center_coords)\n",
    "        improvement_ratio = (seed_time - best_time)/seed_time if seed_time>0 else 0.0\n",
    "        print(f\"‚úÖ Seed time={seed_time:.2f}s best time={best_time:.2f}s improve={improvement_ratio*100:.1f}% dist={best_distance:.2f}m clusters={len(clusters)}\")\n",
    "\n",
    "        cycle = 0\n",
    "        alive_log = []\n",
    "        energy_log = []\n",
    "        cluster_count_log = []\n",
    "        reclustering_cycles = []\n",
    "        reclustering_details = []\n",
    "        death_events = []\n",
    "        reoptimization_count = 0\n",
    "        last_seed_time = seed_time\n",
    "        last_best_time = best_time\n",
    "        last_best_distance = best_distance\n",
    "        stop_reason = \"\"\n",
    "\n",
    "        print(f\"\\n{'='*60}\\nüöÄ B·∫ÆT ƒê·∫¶U M√î PH·ªéNG (PSO)\\n{'='*60}\")\n",
    "\n",
    "        while True:\n",
    "            cycle += 1\n",
    "            alive_log.append(len(all_nodes))\n",
    "            total_energy = sum(all_nodes[n]['residual_energy'] for n in all_nodes)\n",
    "            energy_log.append(total_energy)\n",
    "            cluster_count_log.append(len(clusters))\n",
    "\n",
    "            alive_ratio = len(all_nodes)/total_nodes if total_nodes>0 else 0\n",
    "            if alive_ratio < 0.9:\n",
    "                stop_reason = f\"Alive ratio below 90% ({alive_ratio*100:.2f}%)\"\n",
    "                print(f\"\\nüõë D·ª´ng ·ªü cycle {cycle}: {stop_reason}\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Cycle {cycle} --- | Alive: {alive_ratio*100:.2f}% ({len(all_nodes)}/{total_nodes}) | Energy: {total_energy:.2f}J | Clusters: {len(clusters)}\")\n",
    "            clusters = reselect_cluster_heads(clusters, all_nodes)\n",
    "\n",
    "            update_energy(all_nodes, clusters, last_best_time)\n",
    "\n",
    "            clusters, dead_nodes = remove_dead_nodes(all_nodes, clusters)\n",
    "            if dead_nodes:\n",
    "                print(f\"   ‚ö° {len(dead_nodes)} node(s) ch·∫øt: {dead_nodes}\")\n",
    "                death_events.append({'cycle': cycle,'dead_count': len(dead_nodes),'dead_ids': dead_nodes})\n",
    "                if len(all_nodes) == 0:\n",
    "                    stop_reason = \"All nodes dead\"\n",
    "                    print(\"   ‚ö†Ô∏è Kh√¥ng c√≤n node s·ªëng\")\n",
    "                    break\n",
    "                print(\"   üîß Ph√¢n c·ª•m l·∫°i do node ch·∫øt...\")\n",
    "                clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "                if len(clusters) == 0:\n",
    "                    stop_reason = \"No clusters after reclustering\"\n",
    "                    print(\"   ‚ö†Ô∏è Kh√¥ng c√≤n c·ª•m h·ª£p l·ªá\")\n",
    "                    break\n",
    "                reclustering_cycles.append(cycle)\n",
    "                reclustering_details.append({'cycle': cycle,'cluster_count': len(clusters)})\n",
    "                sorted_keys = sorted(clusters.keys())\n",
    "                centers = [(200,200,400)] + [tuple(clusters[k]['center']) for k in sorted_keys]\n",
    "                center_coords = np.array(centers)\n",
    "                seed_path_new = seed_nearest_neighbor(center_coords)\n",
    "                seed_time_new = travel_time(seed_path_new, center_coords, v_f, v_AUV)\n",
    "                best_path_new, best_time_new = multi_pso_tsp(center_coords, v_f, v_AUV, n_outer=3, n_particles=30, max_iter=80, verbose=False)\n",
    "                best_distance_new = path_distance(best_path_new, center_coords)\n",
    "                reoptimization_count += 1\n",
    "                last_seed_time = seed_time_new\n",
    "                last_best_time = best_time_new\n",
    "                last_best_distance = best_distance_new\n",
    "                print(f\"   ‚úÖ New seed={seed_time_new:.2f}s best={best_time_new:.2f}s improve={(seed_time_new-best_time_new)/seed_time_new*100 if seed_time_new>0 else 0:.1f}% dist={best_distance_new:.2f}m clusters={len(clusters)}\")\n",
    "\n",
    "        if not stop_reason:\n",
    "            stop_reason = \"Loop ended\"\n",
    "\n",
    "        total_dead_nodes = total_nodes - len(all_nodes)\n",
    "\n",
    "        final_clusters_data_for_metrics = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_nodes = np.array([node_positions[nid] for nid in cinfo['nodes']])\n",
    "            final_clusters_data_for_metrics.append((cluster_nodes, cinfo['nodes']))\n",
    "        final_metrics = clustering.calculate_metrics(final_clusters_data_for_metrics) if clusters else {'num_clusters':0,'avg_cluster_size':0,'min_cluster_size':0,'max_cluster_size':0,'avg_intra_distance':0,'max_intra_distance':0,'balance_score':0}\n",
    "\n",
    "        final_total_energy = sum(nd['residual_energy'] for nd in all_nodes.values())\n",
    "        avg_energy_alive_final = final_total_energy/len(all_nodes) if all_nodes else 0.0\n",
    "        per_cluster_energy_final = []\n",
    "        for cid, cinfo in clusters.items():\n",
    "            cluster_energy = sum(all_nodes[nid]['residual_energy'] for nid in cinfo['nodes'] if nid in all_nodes)\n",
    "            per_cluster_energy_final.append({'cluster_id': cid,'nodes': cinfo['nodes'],'cluster_head': cinfo['cluster_head'],'total_energy': cluster_energy})\n",
    "\n",
    "        dead_node_ids = sorted({nid for ev in death_events for nid in ev['dead_ids']})\n",
    "        death_cycle_counts = {ev['cycle']: ev['dead_count'] for ev in death_events}\n",
    "        first_death_cycle = death_events[0]['cycle'] if death_events else None\n",
    "        last_death_cycle = death_events[-1]['cycle'] if death_events else None\n",
    "        mean_deaths_per_event = (total_dead_nodes/len(death_events)) if death_events else 0.0\n",
    "        cycles_until_first_death = first_death_cycle if first_death_cycle is not None else None\n",
    "\n",
    "        meta = {\n",
    "            'input_file': filename,\n",
    "            'method': 'PSO Selective Reclustering',\n",
    "            'strategy': 'Reselect CH each cycle; Recluster+PSO on node death',\n",
    "            'parameters': {\n",
    "                'r_sen': R_SEN,\n",
    "                'max_cluster_size': MAX_SIZE,\n",
    "                'min_cluster_size': MIN_SIZE,\n",
    "                'v_flow': v_f,\n",
    "                'v_AUV': v_AUV,\n",
    "                'pso_particles': 30,\n",
    "                'pso_outer_loops': 3,\n",
    "                'pso_max_iter': 80\n",
    "            },\n",
    "            'lifecycle': {\n",
    "                'cycles_completed': cycle-1,\n",
    "                'stop_reason': stop_reason,\n",
    "                'reoptimization_count': reoptimization_count,\n",
    "                'reclustering_cycles': reclustering_cycles,\n",
    "                'reclustering_details': reclustering_details\n",
    "            },\n",
    "            'nodes_summary': {\n",
    "                'initial_total_nodes': total_nodes,\n",
    "                'final_alive_nodes': len(all_nodes),\n",
    "                'final_alive_ratio': len(all_nodes)/total_nodes if total_nodes>0 else 0,\n",
    "                'total_dead_nodes': total_dead_nodes,\n",
    "                'dead_node_ids': dead_node_ids\n",
    "            },\n",
    "            'path_stats': {\n",
    "                'initial_seed_time': seed_time,\n",
    "                'initial_best_time': best_time,\n",
    "                'initial_improvement_ratio': improvement_ratio,\n",
    "                'final_seed_time': last_seed_time,\n",
    "                'final_best_time': last_best_time,\n",
    "                'final_improvement_ratio': (last_seed_time - last_best_time)/last_seed_time if last_seed_time>0 else 0.0,\n",
    "                'initial_seed_distance': seed_distance,\n",
    "                'initial_best_distance': best_distance,\n",
    "                'final_best_distance': last_best_distance\n",
    "            },\n",
    "            'clustering_initial': initial_metrics,\n",
    "            'clustering_final': final_metrics,\n",
    "            'energy': {\n",
    "                'initial_total_energy': initial_total_energy,\n",
    "                'final_total_energy': final_total_energy,\n",
    "                'avg_energy_alive_final': avg_energy_alive_final,\n",
    "                'per_cluster_energy_final': per_cluster_energy_final\n",
    "            },\n",
    "            'death_events': death_events,\n",
    "            'death_analytics': {\n",
    "                'death_cycle_counts': death_cycle_counts,\n",
    "                'first_death_cycle': first_death_cycle,\n",
    "                'last_death_cycle': last_death_cycle,\n",
    "                'mean_deaths_per_event': mean_deaths_per_event,\n",
    "                'cycles_until_first_death': cycles_until_first_death\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_json = os.path.join(output_folder, f\"result_{filename}\")\n",
    "        with open(output_json,'w') as f: json.dump(meta, f, indent=4)\n",
    "\n",
    "        results_summary.append((filename, cycle-1, len(reclustering_cycles), total_dead_nodes))\n",
    "        print(f\"\\n‚úÖ {filename}: cycles={cycle-1}, recluster={len(reclustering_cycles)}, dead={total_dead_nodes}\")\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.plot(range(len(alive_log)), alive_log, marker='o', linewidth=2, color='steelblue')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Alive nodes - {filename}\"); plt.xlabel(\"Cycle\"); plt.ylabel(\"Alive\"); plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=total_nodes*0.9, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.plot(range(len(energy_log)), energy_log, marker='s', linewidth=2, color='orange')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Total energy - {filename}\"); plt.xlabel(\"Cycle\"); plt.ylabel(\"Energy (J)\"); plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.plot(range(1, len(cluster_count_log)+1), cluster_count_log, marker='^', linewidth=2, color='green')\n",
    "        for rc in reclustering_cycles: plt.axvline(x=rc, color='red', linestyle='--', alpha=0.5)\n",
    "        plt.title(f\"Clusters per cycle - {filename}\"); plt.xlabel(\"Cycle\"); plt.ylabel(\"Clusters\"); plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.axis('off')\n",
    "        alive_ratio_final = len(all_nodes)/total_nodes if total_nodes>0 else 0\n",
    "        death_lines = []\n",
    "        for ev in death_events[:6]: death_lines.append(f\"  ‚Ä¢ C{ev['cycle']}: -{ev['dead_count']}\")\n",
    "        if len(death_events) > 6: death_lines.append(\"  ‚Ä¢ ...\")\n",
    "        death_block = \"\\n\".join(death_lines) if death_lines else \"  ‚Ä¢ (None)\"\n",
    "        info_text = f\"\"\"SUMMARY\\nCycles: {cycle-1}\\nRecluster events: {len(reclustering_cycles)}\\nTotal dead: {total_dead_nodes}\\nAlive: {len(all_nodes)}/{total_nodes} ({alive_ratio_final*100:.1f}%)\\nStop: {stop_reason}\\nDeaths:\\n{death_block}\\nInitial improve: {improvement_ratio*100:.1f}%\\nFinal improve: {(last_seed_time-last_best_time)/last_seed_time*100 if last_seed_time>0 else 0:.1f}%\"\"\"\n",
    "        plt.text(0.02, 0.05, info_text, fontsize=9, family='monospace', va='bottom')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_folder, f\"summary_{filename}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    if results_summary:\n",
    "        print(f\"\\n{'='*60}\\nüìä T·ªîNG K·∫æT\\n{'='*60}\")\n",
    "        for fname, cycles, recount, dead in results_summary:\n",
    "            print(f\"  {fname}: cycles={cycles}, recluster={recount}, dead={dead}\")\n",
    "        print(f\"\\n‚úÖ Ho√†n th√†nh! K·∫øt qu·∫£ t·∫°i: {output_folder}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra\")\n",
    "\n",
    "print(\"‚úì Main function (PSO enriched) loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec634f8-0d7a-403b-80bf-cfdfe71bad34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T18:27:14.247359Z",
     "iopub.status.busy": "2025-11-23T18:27:14.247083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8816109,
     "sourceId": 13841806,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
