{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbdcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# ==========================\n",
    "# HÀM PHỤ TÍNH TOÁN\n",
    "# ==========================\n",
    "\n",
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    Lx, Ly, Lz = x2 - x1, y2 - y1, z2 - z1\n",
    "    L_mag = math.sqrt(Lx**2 + Ly**2 + Lz**2)\n",
    "    if L_mag == 0:\n",
    "        return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta) / v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    v_s = abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "    return v_s\n",
    "\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    total_time = 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i + 1]]\n",
    "        d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        total_time += d / max(v_s, 1e-9)\n",
    "    # quay lại điểm đầu\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    total_time += d / max(v_s, 1e-9)\n",
    "    return total_time\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LỚP CHÍNH GA\n",
    "# ==========================\n",
    "\n",
    "class ClusterTSP_GA:\n",
    "    def __init__(self, clusters, ga_params=None):\n",
    "        self.clusters = clusters\n",
    "        self.cluster_centers = [(0.0, 0.0, 0.0)] + [clusters[k][\"center\"] for k in sorted(clusters.keys())]\n",
    "        self.n = len(self.cluster_centers)\n",
    "\n",
    "        defaults = {\n",
    "            'pop_size': 50,\n",
    "            'generations': 200,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_rate': 0.2,\n",
    "            'elitism_k': 3,\n",
    "            'tournament_size': 3,\n",
    "            'crossover_type': 'OX',\n",
    "            'mutation_type': 'inversion',\n",
    "            'local_search': True,\n",
    "            'v_f': 0.3,\n",
    "            'v_AUV': 1.0,\n",
    "            'verbose': False\n",
    "        }\n",
    "        if ga_params:\n",
    "            defaults.update(ga_params)\n",
    "        self.params = defaults\n",
    "        self.best_fitness_history = []\n",
    "\n",
    "    def create_individual(self):\n",
    "        seq = list(range(1, self.n))\n",
    "        random.shuffle(seq)\n",
    "        return [0] + seq\n",
    "\n",
    "    def create_population(self):\n",
    "        return [self.create_individual() for _ in range(self.params['pop_size'])]\n",
    "\n",
    "    def fitness(self, ind):\n",
    "        total_time = travel_time(ind, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        return 1.0 / (total_time + 1e-9)\n",
    "\n",
    "    def tournament_selection(self, population):\n",
    "        return max(random.sample(population, self.params['tournament_size']), key=self.fitness)\n",
    "\n",
    "    def order_crossover(self, p1, p2):\n",
    "        sub1, sub2 = p1[1:], p2[1:]\n",
    "        a, b = sorted(random.sample(range(len(sub1)), 2))\n",
    "        c1, c2 = [-1]*len(sub1), [-1]*len(sub1)\n",
    "        c1[a:b], c2[a:b] = sub1[a:b], sub2[a:b]\n",
    "        ptr = b\n",
    "        for x in sub2[b:]+sub2[:b]:\n",
    "            if x not in c1:\n",
    "                c1[ptr % len(sub1)] = x\n",
    "                ptr += 1\n",
    "        ptr = b\n",
    "        for x in sub1[b:]+sub1[:b]:\n",
    "            if x not in c2:\n",
    "                c2[ptr % len(sub2)] = x\n",
    "                ptr += 1\n",
    "        return [0]+c1, [0]+c2\n",
    "\n",
    "    def inversion_mutation(self, ind):\n",
    "        i, j = sorted(random.sample(range(1, len(ind)), 2))\n",
    "        ind[i:j+1] = list(reversed(ind[i:j+1]))\n",
    "        return ind\n",
    "\n",
    "    def evolve(self):\n",
    "        pop = self.create_population()\n",
    "        best = max(pop, key=self.fitness)\n",
    "        best_time = travel_time(best, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        for _ in range(self.params['generations']):\n",
    "            fitnesses = [self.fitness(ind) for ind in pop]\n",
    "            best_gen = pop[np.argmax(fitnesses)]\n",
    "            gen_best_time = travel_time(best_gen, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "            if gen_best_time < best_time:\n",
    "                best, best_time = best_gen.copy(), gen_best_time\n",
    "            elite_idx = np.argsort(fitnesses)[-self.params['elitism_k']:]\n",
    "            new_pop = [pop[i].copy() for i in elite_idx]\n",
    "            while len(new_pop) < self.params['pop_size']:\n",
    "                p1, p2 = self.tournament_selection(pop), self.tournament_selection(pop)\n",
    "                c1, c2 = self.order_crossover(p1, p2) if random.random() < self.params['crossover_rate'] else (p1.copy(), p2.copy())\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c1 = self.inversion_mutation(c1)\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c2 = self.inversion_mutation(c2)\n",
    "                new_pop += [c1, c2]\n",
    "            pop = new_pop[:self.params['pop_size']]\n",
    "        return best, best_time\n",
    "\n",
    "\n",
    "def compute_energy(best_time):\n",
    "    G, L, n = 100, 1024, 4\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "    E_rx_TN = G * P_r * L * n / DR\n",
    "    E_tx_TN = G * P_t * L * n / DR_i\n",
    "    E_idle_TN = (best_time - (G * L * n / DR) - (G * L * n / DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "    return {\n",
    "        \"Member\": {\"E_tx\": E_tx_MN, \"E_idle\": E_idle_MN, \"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_rx\": E_rx_TN, \"E_tx\": E_tx_TN, \"E_idle\": E_idle_TN, \"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CHẠY TOÀN BỘ CÁC FILE JSON TRONG THƯ MỤC\n",
    "# ==========================\n",
    "\n",
    "def main():\n",
    "    input_dir = \"D:/Year 4/tiến hóa/project/UWSN_greedy/output_data_kmeans\"\n",
    "    output_dir = \"D:/Year 4/tiến hóa/project/UWSN_greedy/output_path/output_ga/\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith(\".json\")]\n",
    "\n",
    "    ga_params = {\n",
    "        'pop_size': 40,\n",
    "        'generations': 200,\n",
    "        'crossover_rate': 0.8,\n",
    "        'mutation_rate': 0.2,\n",
    "        'elitism_k': 3,\n",
    "        'local_search': True,\n",
    "        'v_f': 1.2,\n",
    "        'v_AUV': 3.0,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        print(f\"\\n=== Đang xử lý file: {filename} ===\")\n",
    "\n",
    "        with open(input_path, 'r') as f:\n",
    "            clusters = json.load(f)\n",
    "\n",
    "        ga = ClusterTSP_GA(clusters, ga_params)\n",
    "        best, best_time = ga.evolve()\n",
    "        energy = compute_energy(best_time)\n",
    "\n",
    "        result = {\n",
    "            \"input_file\": filename,\n",
    "            \"best_path\": best,\n",
    "            \"best_time\": best_time,\n",
    "            \"energy\": energy,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"result_{os.path.splitext(filename)[0]}.json\")\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(result, f, indent=4)\n",
    "\n",
    "        print(f\"Đã lưu kết quả: {output_path}\")\n",
    "        print(f\"   -> Best time: {best_time:.4f}s\\n\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f352b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ==========================\n",
    "# HÀM TÍNH TOÁN TỐC ĐỘ & THỜI GIAN\n",
    "# ==========================\n",
    "\n",
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    Lx, Ly, Lz = x2 - x1, y2 - y1, z2 - z1\n",
    "    L_mag = math.sqrt(Lx**2 + Ly**2 + Lz**2)\n",
    "    if L_mag == 0:\n",
    "        return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta) / v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    v_s = abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "    return v_s\n",
    "\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    total_time = 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i + 1]]\n",
    "        d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        total_time += d / max(v_s, 1e-9)\n",
    "    # quay lại điểm đầu\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    total_time += d / max(v_s, 1e-9)\n",
    "    return total_time\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LỚP GA (đã chỉnh để map index -> cluster_head id)\n",
    "# ==========================\n",
    "\n",
    "class ClusterTSP_GA:\n",
    "    def __init__(self, clusters, ga_params=None):\n",
    "        # clusters: dict {cid: {\"nodes\": [...], \"center\": [x,y,z], \"cluster_head\": nid}}\n",
    "        self.clusters = clusters\n",
    "        # build ordered list of centers and mapping index->cluster_head id\n",
    "        sorted_keys = sorted(clusters.keys(), key=lambda x: int(x))\n",
    "        self.index_to_ch = [None]  # index 0 is AUV base (O)\n",
    "        self.cluster_centers = [(0.0, 0.0, 0.0)]\n",
    "        for k in sorted_keys:\n",
    "            c = clusters[k]['center']\n",
    "            self.cluster_centers.append(tuple(c))\n",
    "            self.index_to_ch.append(clusters[k].get('cluster_head', None))\n",
    "\n",
    "        self.n = len(self.cluster_centers)\n",
    "\n",
    "        defaults = {\n",
    "            'pop_size': 50,\n",
    "            'generations': 200,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_rate': 0.2,\n",
    "            'elitism_k': 3,\n",
    "            'tournament_size': 3,\n",
    "            'crossover_type': 'OX',\n",
    "            'mutation_type': 'inversion',\n",
    "            'local_search': True,\n",
    "            'v_f': 1.2,\n",
    "            'v_AUV': 3.0,\n",
    "            'verbose': False\n",
    "        }\n",
    "        if ga_params:\n",
    "            defaults.update(ga_params)\n",
    "        self.params = defaults\n",
    "        self.best_fitness_history = []\n",
    "\n",
    "    def create_individual(self):\n",
    "        seq = list(range(1, self.n))\n",
    "        random.shuffle(seq)\n",
    "        return [0] + seq\n",
    "\n",
    "    def create_population(self):\n",
    "        return [self.create_individual() for _ in range(self.params['pop_size'])]\n",
    "\n",
    "    def fitness(self, ind):\n",
    "        total_time = travel_time(ind, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        return 1.0 / (total_time + 1e-9)\n",
    "\n",
    "    def tournament_selection(self, population):\n",
    "        return max(random.sample(population, self.params['tournament_size']), key=self.fitness)\n",
    "\n",
    "    def order_crossover(self, p1, p2):\n",
    "        sub1, sub2 = p1[1:], p2[1:]\n",
    "        a, b = sorted(random.sample(range(len(sub1)), 2))\n",
    "        c1, c2 = [-1]*len(sub1), [-1]*len(sub1)\n",
    "        c1[a:b], c2[a:b] = sub1[a:b], sub2[a:b]\n",
    "        ptr = b\n",
    "        for x in sub2[b:]+sub2[:b]:\n",
    "            if x not in c1:\n",
    "                c1[ptr % len(sub1)] = x\n",
    "                ptr += 1\n",
    "        ptr = b\n",
    "        for x in sub1[b:]+sub1[:b]:\n",
    "            if x not in c2:\n",
    "                c2[ptr % len(sub2)] = x\n",
    "                ptr += 1\n",
    "        return [0]+c1, [0]+c2\n",
    "\n",
    "    def inversion_mutation(self, ind):\n",
    "        i, j = sorted(random.sample(range(1, len(ind)), 2))\n",
    "        ind[i:j+1] = list(reversed(ind[i:j+1]))\n",
    "        return ind\n",
    "\n",
    "    def evolve(self):\n",
    "        pop = self.create_population()\n",
    "        best = max(pop, key=self.fitness)\n",
    "        best_time = travel_time(best, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        for _ in range(self.params['generations']):\n",
    "            fitnesses = [self.fitness(ind) for ind in pop]\n",
    "            best_gen = pop[np.argmax(fitnesses)]\n",
    "            gen_best_time = travel_time(best_gen, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "            if gen_best_time < best_time:\n",
    "                best, best_time = best_gen.copy(), gen_best_time\n",
    "            elite_idx = np.argsort(fitnesses)[-self.params['elitism_k']:]\n",
    "            new_pop = [pop[i].copy() for i in elite_idx]\n",
    "            while len(new_pop) < self.params['pop_size']:\n",
    "                p1, p2 = self.tournament_selection(pop), self.tournament_selection(pop)\n",
    "                c1, c2 = self.order_crossover(p1, p2) if random.random() < self.params['crossover_rate'] else (p1.copy(), p2.copy())\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c1 = self.inversion_mutation(c1)\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c2 = self.inversion_mutation(c2)\n",
    "                new_pop += [c1, c2]\n",
    "            pop = new_pop[:self.params['pop_size']]\n",
    "        # convert best (indices) to cluster head node ids sequence (map 0->'O')\n",
    "        mapped_path = ['O' if idx == 0 else self.index_to_ch[idx] for idx in best]\n",
    "        return best, mapped_path, best_time\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM NĂNG LƯỢNG (giữ nguyên công thức bạn cung cấp)\n",
    "# ==========================\n",
    "\n",
    "def compute_energy(best_time):\n",
    "    G, L, n = 100, 1024, 4\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "    E_rx_TN = G * P_r * L * n / DR\n",
    "    E_tx_TN = G * P_t * L * n / DR_i\n",
    "    E_idle_TN = (best_time - (G * L * n / DR) - (G * L * n / DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "    return {\n",
    "        \"Member\": {\"E_tx\": E_tx_MN, \"E_idle\": E_idle_MN, \"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_rx\": E_rx_TN, \"E_tx\": E_tx_TN, \"E_idle\": E_idle_TN, \"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM PHÂN CỤM VÀ CHỌN CLUSTER HEAD (sử dụng code bạn cung cấp)\n",
    "# ==========================\n",
    "def split_isolated_nodes(clusters, r_sen=60, node_data=None):\n",
    "    \"\"\"\n",
    "    Tách node dựa vào khoảng cách tới Cluster Head, không dùng tâm cụm.\n",
    "    Node nào xa hơn r_sen → tách thành cụm riêng.\n",
    "    \"\"\"\n",
    "\n",
    "    new_clusters = []\n",
    "\n",
    "    for c in clusters:\n",
    "        nodes = c[\"nodes\"]\n",
    "        ids = c[\"node_ids\"]\n",
    "\n",
    "        # Lấy cluster head thật sự (không phải center)\n",
    "        head_id = choose_cluster_head(c, node_data)\n",
    "        head_index = ids.index(head_id)\n",
    "        head_pos = nodes[head_index]\n",
    "\n",
    "        # Tính khoảng cách node tới cluster head\n",
    "        dists = np.linalg.norm(nodes - head_pos, axis=1)\n",
    "\n",
    "        # Node trong cụm hợp lệ (gần cluster head)\n",
    "        in_cluster_idx = np.where(dists <= r_sen)[0]\n",
    "        # Node bị tách vì xa head\n",
    "        isolated_idx = np.where(dists > r_sen)[0]\n",
    "\n",
    "        # Cụm giữ lại (có head và các node gần head)\n",
    "        if len(in_cluster_idx) > 0:\n",
    "            new_clusters.append({\n",
    "                \"node_ids\": [ids[i] for i in in_cluster_idx],\n",
    "                \"nodes\": nodes[in_cluster_idx],\n",
    "                \"center\": np.mean(nodes[in_cluster_idx], axis=0),\n",
    "                \"node_data\": {nid: c[\"node_data\"][nid] for nid in [ids[i] for i in in_cluster_idx]},\n",
    "                \"cluster_head\": head_id   # Lưu head luôn\n",
    "            })\n",
    "\n",
    "        # Node bị tách → mỗi node thành cụm riêng\n",
    "        for i in isolated_idx:\n",
    "            nid = ids[i]\n",
    "            node = nodes[i]\n",
    "            new_clusters.append({\n",
    "                \"node_ids\": [nid],\n",
    "                \"nodes\": np.array([node]),\n",
    "                \"center\": node,\n",
    "                \"node_data\": {nid: c[\"node_data\"][nid]},\n",
    "                \"cluster_head\": nid  # node lẻ tự làm head\n",
    "            })\n",
    "\n",
    "    return new_clusters\n",
    "def cluster_split(nodes, node_ids, node_data=None, r_sen=60, R=150, max_depth=50, depth=0):\n",
    "    \"\"\"\n",
    "    Hàm phân cụm đệ quy (Algorithm 1) + giữ danh tính node lẻ (isolated).\n",
    "    \"\"\"\n",
    "\n",
    "    center = np.mean(nodes, axis=0)\n",
    "    dists = np.linalg.norm(nodes - center, axis=1)\n",
    "\n",
    "    # Điều kiện dừng\n",
    "    if (len(nodes) <= R and np.all(dists <= r_sen)) or depth >= max_depth:\n",
    "        return [{\n",
    "            \"node_ids\": node_ids,\n",
    "            \"nodes\": nodes,\n",
    "            \"center\": center,\n",
    "            \"node_data\": node_data if node_data else {}\n",
    "        }]\n",
    "\n",
    "    # weights for distance vs energy\n",
    "    w_dist = 0.5\n",
    "    w_energy = 0.5\n",
    "\n",
    "    # coords -> chuẩn hóa theo phạm vi hiện tại trong cụm\n",
    "    coords = nodes.astype(float)\n",
    "    coord_scale = np.ptp(coords, axis=0).max()  # peak-to-peak của x,y,z\n",
    "    if coord_scale <= 0:\n",
    "        coords_norm = np.zeros_like(coords)\n",
    "    else:\n",
    "        coords_norm = coords / (coord_scale + 1e-9)\n",
    "\n",
    "    # energy feature: lấy residual_energy từ node_data nếu có, else mặc định\n",
    "    if node_data:\n",
    "        energies = np.array([node_data.get(nid, {}).get('residual_energy', 100.0) for nid in node_ids], dtype=float)\n",
    "    else:\n",
    "        energies = np.full(len(node_ids), 100.0, dtype=float)\n",
    "\n",
    "    # chuẩn hóa năng lượng và đảo chiều nếu muốn năng lượng cao => ưu tiên (giá trị nhỏ hơn)\n",
    "    emin, emax = energies.min(), energies.max()\n",
    "    if emax - emin < 1e-9:\n",
    "        e_norm = np.full_like(energies, 0.5)\n",
    "    else:\n",
    "        e_norm = 1.0 - (energies - emin) / (emax - emin)  # in [0,1]; năng lượng lớn => giá trị nhỏ\n",
    "\n",
    "    # ghép feature với trọng số (dùng sqrt để trọng số tương ứng dưới Euclidean)\n",
    "    feat = np.hstack([\n",
    "        coords_norm * np.sqrt(w_dist),\n",
    "        e_norm.reshape(-1, 1) * np.sqrt(w_energy)\n",
    "    ])\n",
    "\n",
    "    # nếu không đủ mẫu để chia tiếp (1 node) thì gán nhãn 0\n",
    "    if len(node_ids) < 2:\n",
    "        labels = np.zeros(len(node_ids), dtype=int)\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(feat)\n",
    "\n",
    "    clusters = []\n",
    "    for i in range(2):\n",
    "        sub_nodes = nodes[labels == i]\n",
    "        sub_ids = [node_ids[j] for j in range(len(node_ids)) if labels[j] == i]\n",
    "\n",
    "        sub_data = {nid: node_data[nid] for nid in sub_ids} if node_data else None\n",
    "\n",
    "        clusters += cluster_split(sub_nodes, sub_ids, sub_data, r_sen, R, max_depth, depth + 1)\n",
    "\n",
    "    # TÁCH NODE LẺ SAU KHI ĐỆ QUY\n",
    "    clusters = split_isolated_nodes(clusters, r_sen=r_sen, node_data=node_data)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def choose_cluster_head(cluster, node_data_dict):\n",
    "    nodes = cluster[\"nodes\"]\n",
    "    center = cluster[\"center\"]\n",
    "    node_ids = cluster[\"node_ids\"]\n",
    "\n",
    "    dists_to_center = np.linalg.norm(nodes - center, axis=1)\n",
    "    max_Q = -1\n",
    "    best_cluster_head = node_ids[0]\n",
    "\n",
    "    for i, node_id in enumerate(node_ids):\n",
    "        if node_id in node_data_dict:\n",
    "            node_info = node_data_dict[node_id]\n",
    "            E_current = node_info.get('residual_energy', 100.0)\n",
    "            E0 = node_info.get('initial_energy', 100.0)\n",
    "            if E_current <= 0:\n",
    "                E_current = 0.1\n",
    "            energy_ratio = E0 / E_current\n",
    "            d_tocenter = dists_to_center[i]\n",
    "            Q = d_tocenter ** energy_ratio\n",
    "            if Q > max_Q:\n",
    "                max_Q = Q\n",
    "                best_cluster_head = node_id\n",
    "        else:\n",
    "            # fallback: choose nearest to center\n",
    "            if i == 0 or dists_to_center[i] < dists_to_center[node_ids.index(best_cluster_head)]:\n",
    "                best_cluster_head = node_id\n",
    "    return best_cluster_head\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM QUẢN LÝ NĂNG LƯỢNG VÀ NODE CHẾT\n",
    "# ==========================\n",
    "\n",
    "def update_energy(all_nodes, clusters, energy_report):\n",
    "    # energy_report has 'Member' and 'Target' E_total\n",
    "    for cid, cinfo in clusters.items():\n",
    "        ch = cinfo.get('cluster_head')\n",
    "        nodes = cinfo.get('nodes', [])\n",
    "        for nid in nodes:\n",
    "            if nid not in all_nodes:\n",
    "                continue\n",
    "            if nid == ch:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Target']['E_total']\n",
    "            else:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Member']['E_total']\n",
    "            # clamp\n",
    "            if all_nodes[nid]['residual_energy'] < 0:\n",
    "                all_nodes[nid]['residual_energy'] = 0.0\n",
    "\n",
    "\n",
    "def remove_dead_nodes(all_nodes, clusters):\n",
    "    dead = [nid for nid, info in list(all_nodes.items()) if info['residual_energy'] <= 0]\n",
    "    for nid in dead:\n",
    "        del all_nodes[nid]\n",
    "    # remove from node_positions will be handled by caller\n",
    "    # clean clusters: remove clusters that have no nodes left\n",
    "    new_clusters = {}\n",
    "    for cid, cinfo in clusters.items():\n",
    "        alive_nodes = [nid for nid in cinfo.get('nodes', []) if nid in all_nodes]\n",
    "        if alive_nodes:\n",
    "            new_c = dict(cinfo)\n",
    "            new_c['nodes'] = alive_nodes\n",
    "            # if cluster head died, will select new CH during recluster\n",
    "            new_clusters[cid] = new_c\n",
    "    return new_clusters, dead\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM PHÂN CỤM LẠI TỪ node_positions\n",
    "# ==========================\n",
    "\n",
    "def recluster(all_nodes, node_positions, r_sen=50, R=20):\n",
    "    ids = sorted(list(all_nodes.keys()))\n",
    "    if len(ids) == 0:\n",
    "        return {}\n",
    "    coords = np.array([node_positions[nid] for nid in ids])\n",
    "    raw_clusters = cluster_split(coords, ids, all_nodes, r_sen=r_sen, R=R)\n",
    "    clusters = {}\n",
    "    for i, c in enumerate(raw_clusters):\n",
    "        center = c['center'].tolist()\n",
    "        node_ids = c['node_ids']\n",
    "        ch = choose_cluster_head(c, all_nodes)\n",
    "        clusters[i] = {'nodes': node_ids, 'center': center, 'cluster_head': ch}\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# MAIN: vòng lặp nhiều chu kỳ\n",
    "# ==========================\n",
    "\n",
    "def main():\n",
    "    input_dir = \"D:/Year 4/tiến hóa/project/data/output_data_kmeans\"\n",
    "    output_dir = \"D:/Year 4/tiến hóa/project/data/output_path/output_ga_multicycle\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "\n",
    "    ga_params = {\n",
    "        'pop_size': 40,\n",
    "        'generations': 200,\n",
    "        'crossover_rate': 0.8,\n",
    "        'mutation_rate': 0.2,\n",
    "        'elitism_k': 3,\n",
    "        'local_search': True,\n",
    "        'v_f': 1.2,\n",
    "        'v_AUV': 3.0,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "    INITIAL_ENERGY = 100.0\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        print(f\"\\n=== Đang xử lý file: {filename} ===\")\n",
    "        with open(input_path, 'r') as f:\n",
    "            clusters_in = json.load(f)\n",
    "\n",
    "        # Build initial node list and try to find node positions\n",
    "        # If there's a separate nodes_positions.json in same folder, use it.\n",
    "        # Else, approximate member node positions by using cluster centers + small noise.\n",
    "        node_positions = {}\n",
    "        all_nodes = {}\n",
    "\n",
    "        # collect all node ids from clusters_in\n",
    "        all_node_ids = set()\n",
    "        for k, v in clusters_in.items():\n",
    "            for nid in v.get('nodes', []):\n",
    "                all_node_ids.add(nid)\n",
    "            # cluster_head too (may duplicate)\n",
    "            ch = v.get('cluster_head')\n",
    "            if ch is not None:\n",
    "                all_node_ids.add(ch)\n",
    "\n",
    "        # attempt to load node positions file (optional)\n",
    "        nodes_pos_file = \"D:/Year 4/tiến hóa/project/data/input_data_evenly_distributed/nodes_20.json\"\n",
    "        if os.path.exists(nodes_pos_file):\n",
    "            try:\n",
    "                with open(nodes_pos_file, 'r', encoding='utf-8') as f:\n",
    "                    nodes_data = json.load(f)\n",
    "            \n",
    "                # Parse từ list of dict sang dict với key là node id\n",
    "                node_positions = {}\n",
    "                for node in nodes_data:\n",
    "                    node_id = node['id']\n",
    "                    node_positions[node_id] = (node['x'], node['y'], node['z'])\n",
    "                \n",
    "                print(f\"Đã nạp node_positions từ {os.path.basename(nodes_pos_file)}\")\n",
    "                print(f\"Số lượng node positions: {len(node_positions)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Không thể đọc {os.path.basename(nodes_pos_file)}, sẽ tạo vị trí gần center. Error:\", e)\n",
    "\n",
    "        # if no node_positions available, approximate\n",
    "        if not node_positions:\n",
    "            for k, v in clusters_in.items():\n",
    "                center = tuple(v.get('center', (0.0, 0.0, 0.0)))\n",
    "                for nid in v.get('nodes', []):\n",
    "                    # small random offset\n",
    "                    offset = np.random.normal(scale=5.0, size=3)\n",
    "                    node_positions[nid] = tuple(np.array(center) + offset)\n",
    "                # ensure cluster head has a position (if listed separately)\n",
    "                ch = v.get('cluster_head')\n",
    "                if ch is not None and ch not in node_positions:\n",
    "                    node_positions[ch] = center\n",
    "            print(\"Tạo giả lập node_positions bằng center + noise (vì không tìm thấy file vị trí)\")\n",
    "\n",
    "        # initialize energy\n",
    "        for nid in list(all_node_ids):\n",
    "            all_nodes[nid] = {'initial_energy': INITIAL_ENERGY, 'residual_energy': INITIAL_ENERGY}\n",
    "\n",
    "        total_nodes = len(all_nodes)\n",
    "        print(f\"Tổng số node ban đầu: {total_nodes}\")\n",
    "\n",
    "        # Start with clusters_in (possibly reformat keys to ints)\n",
    "        clusters = {}\n",
    "        for k, v in clusters_in.items():\n",
    "            clusters[int(k)] = {'nodes': v.get('nodes', []), 'center': v.get('center', []), 'cluster_head': v.get('cluster_head')}\n",
    "\n",
    "        cycle = 0\n",
    "        outputs = []\n",
    "\n",
    "        while True:\n",
    "            cycle += 1\n",
    "            print(f\"\\n--- Cycle {cycle} ---\")\n",
    "\n",
    "            alive_ratio = len(all_nodes) / total_nodes if total_nodes > 0 else 0\n",
    "            print(f\"Tỉ lệ node sống: {alive_ratio*100:.2f}% ({len(all_nodes)}/{total_nodes})\")\n",
    "            if alive_ratio < 0.9:\n",
    "                print(\"Dừng mô phỏng: tỉ lệ node sống < 90%\")\n",
    "                break\n",
    "\n",
    "            if len(clusters) == 0:\n",
    "                print(\"Không còn cụm nào (không còn node). Dừng.\")\n",
    "                break\n",
    "\n",
    "            # run GA on current clusters\n",
    "            ga = ClusterTSP_GA(clusters, ga_params)\n",
    "            best_indices, best_mapped_path, best_time = ga.evolve()\n",
    "\n",
    "            energy_report = compute_energy(best_time)\n",
    "\n",
    "            # update energies\n",
    "            update_energy(all_nodes, clusters, energy_report)\n",
    "\n",
    "            # remove dead nodes\n",
    "            clusters, dead_nodes = remove_dead_nodes(all_nodes, clusters)\n",
    "            # also remove from node_positions\n",
    "            for d in dead_nodes:\n",
    "                if d in node_positions:\n",
    "                    del node_positions[d]\n",
    "\n",
    "            # log output for this cycle\n",
    "            outputs.append({\n",
    "                'cycle': cycle,\n",
    "                'num_clusters': len(clusters),\n",
    "                'best_path_indices': best_indices,\n",
    "                'best_path_node_ids': best_mapped_path,\n",
    "                'best_time': best_time,\n",
    "                'dead_nodes': dead_nodes,\n",
    "                'alive_nodes': len(all_nodes)\n",
    "            })\n",
    "\n",
    "            print(f\"Số cụm hiện tại: {len(clusters)}\")\n",
    "            print(f\"Đường đi (index trong GA): {best_indices}\")\n",
    "            print(f\"Đường đi (node ids, 'O' = base): {best_mapped_path}\")\n",
    "            if dead_nodes:\n",
    "                print(\"Nodes bị loại (chết) trong chu kỳ này:\", dead_nodes)\n",
    "            else:\n",
    "                print(\"Không có node chết ở chu kỳ này.\")\n",
    "\n",
    "            # if still nodes left, recluster and choose CH for next cycle\n",
    "            if len(all_nodes) > 0:\n",
    "                clusters = recluster(all_nodes, node_positions)\n",
    "                # if recluster produced clusters, ensure centers are lists\n",
    "                for k, v in clusters.items():\n",
    "                    clusters[k]['center'] = [float(x) for x in v['center']]\n",
    "\n",
    "        # save outputs to file\n",
    "        out_filename = os.path.join(output_dir, f\"multicycle_result_{os.path.splitext(filename)[0]}.json\")\n",
    "        meta = {\n",
    "            'input_file': filename,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'initial_total_nodes': total_nodes,\n",
    "            'cycles': cycle - 1,\n",
    "            'outputs': outputs\n",
    "        }\n",
    "        with open(out_filename, 'w') as f:\n",
    "            json.dump(meta, f, indent=4)\n",
    "\n",
    "        print(f\"Kết quả đã lưu: {out_filename}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8b7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#claude\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ==========================\n",
    "# HÀM TÍNH TOÁN TỐC ĐỘ & THỜI GIAN\n",
    "# ==========================\n",
    "\n",
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    Lx, Ly, Lz = x2 - x1, y2 - y1, z2 - z1\n",
    "    L_mag = math.sqrt(Lx**2 + Ly**2 + Lz**2)\n",
    "    if L_mag == 0:\n",
    "        return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta) / v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    v_s = abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "    return v_s\n",
    "\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    total_time = 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i + 1]]\n",
    "        d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        total_time += d / max(v_s, 1e-9)\n",
    "    # quay lại điểm đầu\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    total_time += d / max(v_s, 1e-9)\n",
    "    return total_time\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# LỚP GA (đã chỉnh để map index -> cluster_head id)\n",
    "# ==========================\n",
    "\n",
    "class ClusterTSP_GA:\n",
    "    def __init__(self, clusters, ga_params=None):\n",
    "        # clusters: dict {cid: {\"nodes\": [...], \"center\": [x,y,z], \"cluster_head\": nid}}\n",
    "        self.clusters = clusters\n",
    "        # build ordered list of centers and mapping index->cluster_head id\n",
    "        sorted_keys = sorted(clusters.keys(), key=lambda x: int(x))\n",
    "        self.index_to_ch = [None]  # index 0 is AUV base (O)\n",
    "        self.cluster_centers = [(0.0, 0.0, 0.0)]\n",
    "        for k in sorted_keys:\n",
    "            c = clusters[k]['center']\n",
    "            self.cluster_centers.append(tuple(c))\n",
    "            self.index_to_ch.append(clusters[k].get('cluster_head', None))\n",
    "\n",
    "        self.n = len(self.cluster_centers)\n",
    "\n",
    "        defaults = {\n",
    "            'pop_size': 50,\n",
    "            'generations': 200,\n",
    "            'crossover_rate': 0.8,\n",
    "            'mutation_rate': 0.2,\n",
    "            'elitism_k': 3,\n",
    "            'tournament_size': 3,\n",
    "            'crossover_type': 'OX',\n",
    "            'mutation_type': 'inversion',\n",
    "            'local_search': True,\n",
    "            'v_f': 1.2,\n",
    "            'v_AUV': 3.0,\n",
    "            'verbose': False\n",
    "        }\n",
    "        if ga_params:\n",
    "            defaults.update(ga_params)\n",
    "        self.params = defaults\n",
    "        self.best_fitness_history = []\n",
    "\n",
    "    def create_individual(self):\n",
    "        seq = list(range(1, self.n))\n",
    "        random.shuffle(seq)\n",
    "        return [0] + seq\n",
    "\n",
    "    def create_population(self):\n",
    "        return [self.create_individual() for _ in range(self.params['pop_size'])]\n",
    "\n",
    "    def fitness(self, ind):\n",
    "        total_time = travel_time(ind, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        return 1.0 / (total_time + 1e-9)\n",
    "\n",
    "    def tournament_selection(self, population):\n",
    "        return max(random.sample(population, self.params['tournament_size']), key=self.fitness)\n",
    "\n",
    "    def order_crossover(self, p1, p2):\n",
    "        sub1, sub2 = p1[1:], p2[1:]\n",
    "        a, b = sorted(random.sample(range(len(sub1)), 2))\n",
    "        c1, c2 = [-1]*len(sub1), [-1]*len(sub1)\n",
    "        c1[a:b], c2[a:b] = sub1[a:b], sub2[a:b]\n",
    "        ptr = b\n",
    "        for x in sub2[b:]+sub2[:b]:\n",
    "            if x not in c1:\n",
    "                c1[ptr % len(sub1)] = x\n",
    "                ptr += 1\n",
    "        ptr = b\n",
    "        for x in sub1[b:]+sub1[:b]:\n",
    "            if x not in c2:\n",
    "                c2[ptr % len(sub2)] = x\n",
    "                ptr += 1\n",
    "        return [0]+c1, [0]+c2\n",
    "\n",
    "    def inversion_mutation(self, ind):\n",
    "        i, j = sorted(random.sample(range(1, len(ind)), 2))\n",
    "        ind[i:j+1] = list(reversed(ind[i:j+1]))\n",
    "        return ind\n",
    "\n",
    "    def evolve(self):\n",
    "        pop = self.create_population()\n",
    "        best = max(pop, key=self.fitness)\n",
    "        best_time = travel_time(best, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "        for _ in range(self.params['generations']):\n",
    "            fitnesses = [self.fitness(ind) for ind in pop]\n",
    "            best_gen = pop[np.argmax(fitnesses)]\n",
    "            gen_best_time = travel_time(best_gen, self.cluster_centers, self.params['v_f'], self.params['v_AUV'])\n",
    "            if gen_best_time < best_time:\n",
    "                best, best_time = best_gen.copy(), gen_best_time\n",
    "            elite_idx = np.argsort(fitnesses)[-self.params['elitism_k']:]\n",
    "            new_pop = [pop[i].copy() for i in elite_idx]\n",
    "            while len(new_pop) < self.params['pop_size']:\n",
    "                p1, p2 = self.tournament_selection(pop), self.tournament_selection(pop)\n",
    "                c1, c2 = self.order_crossover(p1, p2) if random.random() < self.params['crossover_rate'] else (p1.copy(), p2.copy())\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c1 = self.inversion_mutation(c1)\n",
    "                if random.random() < self.params['mutation_rate']:\n",
    "                    c2 = self.inversion_mutation(c2)\n",
    "                new_pop += [c1, c2]\n",
    "            pop = new_pop[:self.params['pop_size']]\n",
    "        # convert best (indices) to cluster head node ids sequence (map 0->'O')\n",
    "        mapped_path = ['O' if idx == 0 else self.index_to_ch[idx] for idx in best]\n",
    "        return best, mapped_path, best_time\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM NĂNG LƯỢNG (giữ nguyên công thức bạn cung cấp)\n",
    "# ==========================\n",
    "\n",
    "def compute_energy(best_time):\n",
    "    G, L, n = 100, 1024, 4\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "    E_rx_TN = G * P_r * L * n / DR\n",
    "    E_tx_TN = G * P_t * L * n / DR_i\n",
    "    E_idle_TN = (best_time - (G * L * n / DR) - (G * L * n / DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "    return {\n",
    "        \"Member\": {\"E_tx\": E_tx_MN, \"E_idle\": E_idle_MN, \"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_rx\": E_rx_TN, \"E_tx\": E_tx_TN, \"E_idle\": E_idle_TN, \"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM PHÂN CỤM VÀ CHỌN CLUSTER HEAD (sử dụng code bạn cung cấp)\n",
    "# ==========================\n",
    "def split_isolated_nodes(clusters, r_sen=60, node_data=None):\n",
    "    \"\"\"\n",
    "    Tách node dựa vào khoảng cách tới Cluster Head, không dùng tâm cụm.\n",
    "    Node nào xa hơn r_sen → tách thành cụm riêng.\n",
    "    \"\"\"\n",
    "\n",
    "    new_clusters = []\n",
    "\n",
    "    for c in clusters:\n",
    "        nodes = c[\"nodes\"]\n",
    "        ids = c[\"node_ids\"]\n",
    "\n",
    "        # Lấy cluster head thật sự (không phải center)\n",
    "        head_id = choose_cluster_head(c, node_data)\n",
    "        head_index = ids.index(head_id)\n",
    "        head_pos = nodes[head_index]\n",
    "\n",
    "        # Tính khoảng cách node tới cluster head\n",
    "        dists = np.linalg.norm(nodes - head_pos, axis=1)\n",
    "\n",
    "        # Node trong cụm hợp lệ (gần cluster head)\n",
    "        in_cluster_idx = np.where(dists <= r_sen)[0]\n",
    "        # Node bị tách vì xa head\n",
    "        isolated_idx = np.where(dists > r_sen)[0]\n",
    "\n",
    "        # Cụm giữ lại (có head và các node gần head)\n",
    "        if len(in_cluster_idx) > 0:\n",
    "            new_clusters.append({\n",
    "                \"node_ids\": [ids[i] for i in in_cluster_idx],\n",
    "                \"nodes\": nodes[in_cluster_idx],\n",
    "                \"center\": np.mean(nodes[in_cluster_idx], axis=0),\n",
    "                \"node_data\": {nid: c[\"node_data\"][nid] for nid in [ids[i] for i in in_cluster_idx]},\n",
    "                \"cluster_head\": head_id   # Lưu head luôn\n",
    "            })\n",
    "\n",
    "        # Node bị tách → mỗi node thành cụm riêng\n",
    "        for i in isolated_idx:\n",
    "            nid = ids[i]\n",
    "            node = nodes[i]\n",
    "            new_clusters.append({\n",
    "                \"node_ids\": [nid],\n",
    "                \"nodes\": np.array([node]),\n",
    "                \"center\": node,\n",
    "                \"node_data\": {nid: c[\"node_data\"][nid]},\n",
    "                \"cluster_head\": nid  # node lẻ tự làm head\n",
    "            })\n",
    "\n",
    "    return new_clusters\n",
    "\n",
    "def cluster_split(nodes, node_ids, node_data=None, r_sen=60, R=150, max_depth=50, depth=0):\n",
    "    \"\"\"\n",
    "    Hàm phân cụm đệ quy (Algorithm 1) + giữ danh tính node lẻ (isolated).\n",
    "    \"\"\"\n",
    "\n",
    "    center = np.mean(nodes, axis=0)\n",
    "    dists = np.linalg.norm(nodes - center, axis=1)\n",
    "\n",
    "    # Điều kiện dừng\n",
    "    if (len(nodes) <= R and np.all(dists <= r_sen)) or depth >= max_depth:\n",
    "        return [{\n",
    "            \"node_ids\": node_ids,\n",
    "            \"nodes\": nodes,\n",
    "            \"center\": center,\n",
    "            \"node_data\": node_data if node_data else {}\n",
    "        }]\n",
    "\n",
    "    # weights for distance vs energy\n",
    "    w_dist = 0.5\n",
    "    w_energy = 0.5\n",
    "\n",
    "    # coords -> chuẩn hóa theo phạm vi hiện tại trong cụm\n",
    "    coords = nodes.astype(float)\n",
    "    coord_scale = np.ptp(coords, axis=0).max()  # peak-to-peak của x,y,z\n",
    "    if coord_scale <= 0:\n",
    "        coords_norm = np.zeros_like(coords)\n",
    "    else:\n",
    "        coords_norm = coords / (coord_scale + 1e-9)\n",
    "\n",
    "    # energy feature: lấy residual_energy từ node_data nếu có, else mặc định\n",
    "    if node_data:\n",
    "        energies = np.array([node_data.get(nid, {}).get('residual_energy', 100.0) for nid in node_ids], dtype=float)\n",
    "    else:\n",
    "        energies = np.full(len(node_ids), 100.0, dtype=float)\n",
    "\n",
    "    # chuẩn hóa năng lượng và đảo chiều nếu muốn năng lượng cao => ưu tiên (giá trị nhỏ hơn)\n",
    "    emin, emax = energies.min(), energies.max()\n",
    "    if emax - emin < 1e-9:\n",
    "        e_norm = np.full_like(energies, 0.5)\n",
    "    else:\n",
    "        e_norm = 1.0 - (energies - emin) / (emax - emin)  # in [0,1]; năng lượng lớn => giá trị nhỏ\n",
    "\n",
    "    # ghép feature với trọng số (dùng sqrt để trọng số tương ứng dưới Euclidean)\n",
    "    feat = np.hstack([\n",
    "        coords_norm * np.sqrt(w_dist),\n",
    "        e_norm.reshape(-1, 1) * np.sqrt(w_energy)\n",
    "    ])\n",
    "\n",
    "    # nếu không đủ mẫu để chia tiếp (1 node) thì gán nhãn 0\n",
    "    if len(node_ids) < 2:\n",
    "        labels = np.zeros(len(node_ids), dtype=int)\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(feat)\n",
    "\n",
    "    clusters = []\n",
    "    for i in range(2):\n",
    "        sub_nodes = nodes[labels == i]\n",
    "        sub_ids = [node_ids[j] for j in range(len(node_ids)) if labels[j] == i]\n",
    "\n",
    "        sub_data = {nid: node_data[nid] for nid in sub_ids} if node_data else None\n",
    "\n",
    "        clusters += cluster_split(sub_nodes, sub_ids, sub_data, r_sen, R, max_depth, depth + 1)\n",
    "\n",
    "    # TÁCH NODE LẺ SAU KHI ĐỆ QUY\n",
    "    clusters = split_isolated_nodes(clusters, r_sen=r_sen, node_data=node_data)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def choose_cluster_head(cluster, node_data_dict):\n",
    "    nodes = cluster[\"nodes\"]\n",
    "    center = cluster[\"center\"]\n",
    "    node_ids = cluster[\"node_ids\"]\n",
    "\n",
    "    dists_to_center = np.linalg.norm(nodes - center, axis=1)\n",
    "    max_Q = -1\n",
    "    best_cluster_head = node_ids[0]\n",
    "\n",
    "    for i, node_id in enumerate(node_ids):\n",
    "        if node_id in node_data_dict:\n",
    "            node_info = node_data_dict[node_id]\n",
    "            E_current = node_info.get('residual_energy', 100.0)\n",
    "            E0 = node_info.get('initial_energy', 100.0)\n",
    "            if E_current <= 0:\n",
    "                E_current = 0.1\n",
    "            energy_ratio = E0 / E_current\n",
    "            d_tocenter = dists_to_center[i]\n",
    "            Q = d_tocenter ** energy_ratio\n",
    "            if Q > max_Q:\n",
    "                max_Q = Q\n",
    "                best_cluster_head = node_id\n",
    "        else:\n",
    "            # fallback: choose nearest to center\n",
    "            if i == 0 or dists_to_center[i] < dists_to_center[node_ids.index(best_cluster_head)]:\n",
    "                best_cluster_head = node_id\n",
    "    return best_cluster_head\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM QUẢN LÝ NĂNG LƯỢNG VÀ NODE CHẾT\n",
    "# ==========================\n",
    "\n",
    "def update_energy(all_nodes, clusters, energy_report):\n",
    "    # energy_report has 'Member' and 'Target' E_total\n",
    "    for cid, cinfo in clusters.items():\n",
    "        ch = cinfo.get('cluster_head')\n",
    "        nodes = cinfo.get('nodes', [])\n",
    "        for nid in nodes:\n",
    "            if nid not in all_nodes:\n",
    "                continue\n",
    "            if nid == ch:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Target']['E_total']\n",
    "            else:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Member']['E_total']\n",
    "            # clamp\n",
    "            if all_nodes[nid]['residual_energy'] < 0:\n",
    "                all_nodes[nid]['residual_energy'] = 0.0\n",
    "\n",
    "\n",
    "def remove_dead_nodes(all_nodes, clusters):\n",
    "    dead = [nid for nid, info in list(all_nodes.items()) if info['residual_energy'] <= 0]\n",
    "    for nid in dead:\n",
    "        del all_nodes[nid]\n",
    "    # remove from node_positions will be handled by caller\n",
    "    # clean clusters: remove clusters that have no nodes left\n",
    "    new_clusters = {}\n",
    "    for cid, cinfo in clusters.items():\n",
    "        alive_nodes = [nid for nid in cinfo.get('nodes', []) if nid in all_nodes]\n",
    "        if alive_nodes:\n",
    "            new_c = dict(cinfo)\n",
    "            new_c['nodes'] = alive_nodes\n",
    "            # if cluster head died, will select new CH during recluster\n",
    "            new_clusters[cid] = new_c\n",
    "    return new_clusters, dead\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM PHÂN CỤM LẠI TỪ node_positions\n",
    "# ==========================\n",
    "\n",
    "def recluster(all_nodes, node_positions, r_sen=50, R=20):\n",
    "    ids = sorted(list(all_nodes.keys()))\n",
    "    if len(ids) == 0:\n",
    "        return {}\n",
    "    coords = np.array([node_positions[nid] for nid in ids])\n",
    "    raw_clusters = cluster_split(coords, ids, all_nodes, r_sen=r_sen, R=R)\n",
    "    clusters = {}\n",
    "    for i, c in enumerate(raw_clusters):\n",
    "        center = c['center'].tolist()\n",
    "        node_ids = c['node_ids']\n",
    "        ch = choose_cluster_head(c, all_nodes)\n",
    "        clusters[i] = {'nodes': node_ids, 'center': center, 'cluster_head': ch}\n",
    "    return clusters\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# HÀM VẼ BIỂU ĐỒ PHÂN TÍCH\n",
    "# ==========================\n",
    "\n",
    "def plot_network_analysis(outputs, total_nodes, INITIAL_ENERGY, filename, output_dir):\n",
    "    \"\"\"\n",
    "    Vẽ 4 biểu đồ phân tích:\n",
    "    1. Số node sống theo chu kỳ\n",
    "    2. Số node chết tích lũy theo chu kỳ\n",
    "    3. Tổng năng lượng mạng theo chu kỳ\n",
    "    4. Thời gian di chuyển AUV theo chu kỳ\n",
    "    \"\"\"\n",
    "    \n",
    "    cycles = [o['cycle'] for o in outputs]\n",
    "    alive_nodes = [o['alive_nodes'] for o in outputs]\n",
    "    \n",
    "    # Tính số node chết tích lũy\n",
    "    cumulative_dead = [total_nodes - a for a in alive_nodes]\n",
    "    \n",
    "    # Tính tổng năng lượng còn lại (ước tính)\n",
    "    # Giả sử mỗi node bắt đầu với INITIAL_ENERGY\n",
    "    # Năng lượng tiêu thụ trung bình mỗi chu kỳ\n",
    "    energy_consumed_per_cycle = []\n",
    "    total_energy_remaining = []\n",
    "    \n",
    "    for i, o in enumerate(outputs):\n",
    "        # Ước tính năng lượng còn lại dựa trên số node sống\n",
    "        energy_remaining = o['alive_nodes'] * INITIAL_ENERGY * (1 - (i / len(outputs)))\n",
    "        total_energy_remaining.append(energy_remaining)\n",
    "    \n",
    "    # Thời gian di chuyển\n",
    "    travel_times = [o['best_time'] for o in outputs]\n",
    "    \n",
    "    # Tìm chu kỳ đầu tiên có node chết\n",
    "    first_death_cycle = None\n",
    "    for i, o in enumerate(outputs):\n",
    "        if o['dead_nodes']:\n",
    "            first_death_cycle = o['cycle']\n",
    "            break\n",
    "    \n",
    "    # Tạo figure với 4 subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(f'Phân tích mạng UWSN - {filename}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Biểu đồ 1: Số node sống\n",
    "    axes[0, 0].plot(cycles, alive_nodes, 'b-o', linewidth=2, markersize=6)\n",
    "    axes[0, 0].axhline(y=total_nodes * 0.9, color='r', linestyle='--', \n",
    "                       label=f'Ngưỡng 90% ({int(total_nodes * 0.9)} nodes)')\n",
    "    axes[0, 0].set_xlabel('Chu kỳ', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Số node sống', fontsize=12)\n",
    "    axes[0, 0].set_title('Số node sống theo thời gian', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].set_ylim(bottom=0)\n",
    "    \n",
    "    # Biểu đồ 2: Số node chết tích lũy\n",
    "    axes[0, 1].plot(cycles, cumulative_dead, 'r-s', linewidth=2, markersize=6)\n",
    "    axes[0, 1].set_xlabel('Chu kỳ', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Số node chết (tích lũy)', fontsize=12)\n",
    "    axes[0, 1].set_title('Số node chết tích lũy theo thời gian', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim(bottom=0)\n",
    "    \n",
    "    # Đánh dấu chu kỳ đầu tiên có node chết\n",
    "    if first_death_cycle:\n",
    "        idx = first_death_cycle - 1\n",
    "        axes[0, 1].axvline(x=first_death_cycle, color='orange', linestyle='--', \n",
    "                          label=f'Chu kỳ đầu tiên có node chết: {first_death_cycle}')\n",
    "        axes[0, 1].legend()\n",
    "    \n",
    "    # Biểu đồ 3: Tổng năng lượng mạng\n",
    "    axes[1, 0].plot(cycles, total_energy_remaining, 'g-^', linewidth=2, markersize=6)\n",
    "    axes[1, 0].set_xlabel('Chu kỳ', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Tổng năng lượng còn lại (J)', fontsize=12)\n",
    "    axes[1, 0].set_title('Năng lượng tổng thể mạng theo thời gian', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim(bottom=0)\n",
    "    \n",
    "    # Biểu đồ 4: Thời gian di chuyển AUV\n",
    "    axes[1, 1].plot(cycles, travel_times, 'm-d', linewidth=2, markersize=6)\n",
    "    axes[1, 1].set_xlabel('Chu kỳ', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Thời gian di chuyển (s)', fontsize=12)\n",
    "    axes[1, 1].set_title('Thời gian chu kỳ AUV', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Thêm thống kê\n",
    "    avg_time = np.mean(travel_times)\n",
    "    axes[1, 1].axhline(y=avg_time, color='r', linestyle='--', \n",
    "                      label=f'Trung bình: {avg_time:.2f}s')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Lưu biểu đồ\n",
    "    chart_filename = os.path.join(output_dir, f\"analysis_{os.path.splitext(filename)[0]}.png\")\n",
    "    plt.savefig(chart_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Đã lưu biểu đồ phân tích: {chart_filename}\")\n",
    "    \n",
    "    # Hiển thị biểu đồ\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return first_death_cycle\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# MAIN: vòng lặp nhiều chu kỳ\n",
    "# ==========================\n",
    "\n",
    "def main():\n",
    "    input_dir = \"D:/Year 4/tiến hóa/project/data/output_data_kmeans\"\n",
    "    output_dir = \"D:/Year 4/tiến hóa/project/data/output_path/output_ga_multicycle\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "\n",
    "    ga_params = {\n",
    "        'pop_size': 40,\n",
    "        'generations': 200,\n",
    "        'crossover_rate': 0.8,\n",
    "        'mutation_rate': 0.2,\n",
    "        'elitism_k': 3,\n",
    "        'local_search': True,\n",
    "        'v_f': 1.2,\n",
    "        'v_AUV': 3.0,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "    INITIAL_ENERGY = 100.0\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        print(f\"\\n=== Đang xử lý file: {filename} ===\")\n",
    "        with open(input_path, 'r') as f:\n",
    "            clusters_in = json.load(f)\n",
    "\n",
    "        # Build initial node list and try to find node positions\n",
    "        node_positions = {}\n",
    "        all_nodes = {}\n",
    "\n",
    "        # collect all node ids from clusters_in\n",
    "        all_node_ids = set()\n",
    "        for k, v in clusters_in.items():\n",
    "            for nid in v.get('nodes', []):\n",
    "                all_node_ids.add(nid)\n",
    "            ch = v.get('cluster_head')\n",
    "            if ch is not None:\n",
    "                all_node_ids.add(ch)\n",
    "\n",
    "        # attempt to load node positions file (optional)\n",
    "        nodes_pos_file = \"D:/Year 4/tiến hóa/project/data/input_data_evenly_distributed/nodes_20.json\"\n",
    "        if os.path.exists(nodes_pos_file):\n",
    "            try:\n",
    "                with open(nodes_pos_file, 'r', encoding='utf-8') as f:\n",
    "                    nodes_data = json.load(f)\n",
    "            \n",
    "                # Parse từ list of dict sang dict với key là node id\n",
    "                node_positions = {}\n",
    "                for node in nodes_data:\n",
    "                    node_id = node['id']\n",
    "                    node_positions[node_id] = (node['x'], node['y'], node['z'])\n",
    "                \n",
    "                print(f\"Đã nạp node_positions từ {os.path.basename(nodes_pos_file)}\")\n",
    "                print(f\"Số lượng node positions: {len(node_positions)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Không thể đọc {os.path.basename(nodes_pos_file)}, sẽ tạo vị trí gần center. Error:\", e)\n",
    "\n",
    "        # if no node_positions available, approximate\n",
    "        if not node_positions:\n",
    "            for k, v in clusters_in.items():\n",
    "                center = tuple(v.get('center', (0.0, 0.0, 0.0)))\n",
    "                for nid in v.get('nodes', []):\n",
    "                    offset = np.random.normal(scale=5.0, size=3)\n",
    "                    node_positions[nid] = tuple(np.array(center) + offset)\n",
    "                ch = v.get('cluster_head')\n",
    "                if ch is not None and ch not in node_positions:\n",
    "                    node_positions[ch] = center\n",
    "            print(\"Tạo giả lập node_positions bằng center + noise (vì không tìm thấy file vị trí)\")\n",
    "\n",
    "        # initialize energy\n",
    "        for nid in list(all_node_ids):\n",
    "            all_nodes[nid] = {'initial_energy': INITIAL_ENERGY, 'residual_energy': INITIAL_ENERGY}\n",
    "\n",
    "        total_nodes = len(all_nodes)\n",
    "        print(f\"Tổng số node ban đầu: {total_nodes}\")\n",
    "\n",
    "        # Start with clusters_in\n",
    "        clusters = {}\n",
    "        for k, v in clusters_in.items():\n",
    "            clusters[int(k)] = {'nodes': v.get('nodes', []), 'center': v.get('center', []), \n",
    "                               'cluster_head': v.get('cluster_head')}\n",
    "\n",
    "        cycle = 0\n",
    "        outputs = []\n",
    "        cumulative_time = 0.0  # Tổng thời gian tích lũy\n",
    "\n",
    "        while True:\n",
    "            cycle += 1\n",
    "            print(f\"\\n--- Cycle {cycle} ---\")\n",
    "\n",
    "            alive_ratio = len(all_nodes) / total_nodes if total_nodes > 0 else 0\n",
    "            print(f\"Tỉ lệ node sống: {alive_ratio*100:.2f}% ({len(all_nodes)}/{total_nodes})\")\n",
    "            if alive_ratio < 0.9:\n",
    "                print(\"Dừng mô phỏng: tỉ lệ node sống < 90%\")\n",
    "                break\n",
    "\n",
    "            if len(clusters) == 0:\n",
    "                print(\"Không còn cụm nào (không còn node). Dừng.\")\n",
    "                break\n",
    "\n",
    "            # run GA on current clusters\n",
    "            ga = ClusterTSP_GA(clusters, ga_params)\n",
    "            best_indices, best_mapped_path, best_time = ga.evolve()\n",
    "\n",
    "            # Cộng dồn thời gian\n",
    "            cumulative_time += best_time\n",
    "\n",
    "            energy_report = compute_energy(best_time)\n",
    "\n",
    "            # update energies\n",
    "            update_energy(all_nodes, clusters, energy_report)\n",
    "\n",
    "            # remove dead nodes\n",
    "            clusters, dead_nodes = remove_dead_nodes(all_nodes, clusters)\n",
    "            # also remove from node_positions\n",
    "            for d in dead_nodes:\n",
    "                if d in node_positions:\n",
    "                    del node_positions[d]\n",
    "\n",
    "            # Tính tổng năng lượng còn lại trong mạng\n",
    "            total_energy_remaining = sum(node['residual_energy'] for node in all_nodes.values())\n",
    "\n",
    "            # log output for this cycle\n",
    "            outputs.append({\n",
    "                'cycle': cycle,\n",
    "                'num_clusters': len(clusters),\n",
    "                'best_path_indices': best_indices,\n",
    "                'best_path_node_ids': best_mapped_path,\n",
    "                'best_time': best_time,\n",
    "                'cumulative_time': cumulative_time,\n",
    "                'dead_nodes': dead_nodes,\n",
    "                'alive_nodes': len(all_nodes),\n",
    "                'total_energy_remaining': total_energy_remaining\n",
    "            })\n",
    "\n",
    "            print(f\"Số cụm hiện tại: {len(clusters)}\")\n",
    "            print(f\"Đường đi (index trong GA): {best_indices}\")\n",
    "            print(f\"Đường đi (node ids, 'O' = base): {best_mapped_path}\")\n",
    "            print(f\"Thời gian chu kỳ này: {best_time:.2f}s\")\n",
    "            print(f\"Tổng thời gian tích lũy: {cumulative_time:.2f}s\")\n",
    "            print(f\"Tổng năng lượng còn lại: {total_energy_remaining:.2f}J\")\n",
    "            \n",
    "            if dead_nodes:\n",
    "                print(f\"Nodes bị loại (chết) trong chu kỳ này: {dead_nodes} (Số lượng: {len(dead_nodes)})\")\n",
    "            else:\n",
    "                print(\"Không có node chết ở chu kỳ này.\")\n",
    "\n",
    "            # if still nodes left, recluster and choose CH for next cycle\n",
    "            if len(all_nodes) > 0:\n",
    "                clusters = recluster(all_nodes, node_positions)\n",
    "                # if recluster produced clusters, ensure centers are lists\n",
    "                for k, v in clusters.items():\n",
    "                    clusters[k]['center'] = [float(x) for x in v['center']]\n",
    "\n",
    "        # Phân tích và tìm chu kỳ đầu tiên có node chết\n",
    "        first_death_cycle = None\n",
    "        first_death_time = None\n",
    "        \n",
    "        for o in outputs:\n",
    "            if o['dead_nodes']:\n",
    "                first_death_cycle = o['cycle']\n",
    "                first_death_time = o['cumulative_time']\n",
    "                break\n",
    "        \n",
    "        # In thông tin phân tích\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHÂN TÍCH KẾT QUẢ MÔ PHỎNG\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Tổng số chu kỳ: {cycle - 1}\")\n",
    "        print(f\"Tổng thời gian hoạt động: {cumulative_time:.2f}s ({cumulative_time/3600:.2f} giờ)\")\n",
    "        \n",
    "        if first_death_cycle:\n",
    "            print(f\"\\nChu kỳ đầu tiên có node chết: Chu kỳ {first_death_cycle}\")\n",
    "            print(f\"Thời gian đến khi có node chết đầu tiên: {first_death_time:.2f}s ({first_death_time/3600:.2f} giờ)\")\n",
    "            print(f\"Tỷ lệ thời gian: {(first_death_time/cumulative_time)*100:.2f}% tổng thời gian\")\n",
    "        else:\n",
    "            print(\"\\nKhông có node nào chết trong quá trình mô phỏng\")\n",
    "        \n",
    "        print(f\"\\nSố node còn sống cuối cùng: {len(all_nodes)}/{total_nodes}\")\n",
    "        print(f\"Số node đã chết: {total_nodes - len(all_nodes)}\")\n",
    "        print(f\"Tỷ lệ sống sót: {(len(all_nodes)/total_nodes)*100:.2f}%\")\n",
    "        \n",
    "        # Thống kê về thời gian chu kỳ\n",
    "        cycle_times = [o['best_time'] for o in outputs]\n",
    "        print(f\"\\nThời gian chu kỳ trung bình: {np.mean(cycle_times):.2f}s\")\n",
    "        print(f\"Thời gian chu kỳ ngắn nhất: {np.min(cycle_times):.2f}s\")\n",
    "        print(f\"Thời gian chu kỳ dài nhất: {np.max(cycle_times):.2f}s\")\n",
    "        print(f\"Độ lệch chuẩn: {np.std(cycle_times):.2f}s\")\n",
    "        \n",
    "        # Thống kê về số node chết mỗi chu kỳ\n",
    "        deaths_per_cycle = [len(o['dead_nodes']) for o in outputs if o['dead_nodes']]\n",
    "        if deaths_per_cycle:\n",
    "            print(f\"\\nSố node chết trung bình mỗi chu kỳ (khi có chết): {np.mean(deaths_per_cycle):.2f}\")\n",
    "            print(f\"Số node chết nhiều nhất trong 1 chu kỳ: {np.max(deaths_per_cycle)}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # save outputs to file\n",
    "        out_filename = os.path.join(output_dir, f\"multicycle_result_{os.path.splitext(filename)[0]}.json\")\n",
    "        meta = {\n",
    "            'input_file': filename,\n",
    "            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'initial_total_nodes': total_nodes,\n",
    "            'cycles': cycle - 1,\n",
    "            'total_operation_time': cumulative_time,\n",
    "            'first_death_cycle': first_death_cycle,\n",
    "            'first_death_time': first_death_time,\n",
    "            'final_alive_nodes': len(all_nodes),\n",
    "            'survival_rate': (len(all_nodes)/total_nodes)*100,\n",
    "            'outputs': outputs\n",
    "        }\n",
    "        with open(out_filename, 'w') as f:\n",
    "            json.dump(meta, f, indent=4)\n",
    "\n",
    "        print(f\"\\nKết quả đã lưu: {out_filename}\")\n",
    "        \n",
    "        # Vẽ biểu đồ phân tích\n",
    "        print(\"\\nĐang tạo biểu đồ phân tích...\")\n",
    "        plot_network_analysis(outputs, total_nodes, INITIAL_ENERGY, filename, output_dir)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Hoàn thành xử lý file: {filename}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
