{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T02:42:31.187773Z",
     "iopub.status.busy": "2025-11-04T02:42:31.187276Z",
     "iopub.status.idle": "2025-11-04T02:46:28.168378Z",
     "shell.execute_reply": "2025-11-04T02:46:28.167249Z",
     "shell.execute_reply.started": "2025-11-04T02:42:31.187736Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Import class Clustering t·ª´ cluster.py\n",
    "from cluster import Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_vs(p1, p2, v_f, v_AUV):\n",
    "    x1, y1, z1 = p1\n",
    "    x2, y2, z2 = p2\n",
    "    Lx, Ly, Lz = x2 - x1, y2 - y1, z2 - z1\n",
    "    L_mag = math.sqrt(Lx**2 + Ly**2 + Lz**2)\n",
    "    if L_mag == 0:\n",
    "        return v_AUV\n",
    "    cos_beta = Lz / L_mag\n",
    "    cos_beta = np.clip(cos_beta, -1, 1)\n",
    "    beta = math.acos(cos_beta)\n",
    "    inner = np.clip((v_f * cos_beta) / v_AUV, -1, 1)\n",
    "    angle = beta + math.acos(inner)\n",
    "    if abs(cos_beta) < 1e-9:\n",
    "        return v_AUV\n",
    "    return abs(math.cos(angle) * v_AUV / cos_beta)\n",
    "\n",
    "def travel_time(path, coords, v_f, v_AUV):\n",
    "    total_time = 0.0\n",
    "    if len(path) <= 1:\n",
    "        return 0.0\n",
    "    for i in range(len(path) - 1):\n",
    "        p1, p2 = coords[path[i]], coords[path[i + 1]]\n",
    "        d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "        v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "        total_time += d / max(v_s, 1e-9)\n",
    "    # return to start\n",
    "    p1, p2 = coords[path[-1]], coords[path[0]]\n",
    "    d = np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "    v_s = compute_vs(tuple(p1), tuple(p2), v_f, v_AUV)\n",
    "    total_time += d / max(v_s, 1e-9)\n",
    "    return total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy(best_time, n_members):\n",
    "    \"\"\"\n",
    "    T√≠nh nƒÉng l∆∞·ª£ng ti√™u th·ª• cho Member Node v√† Cluster Head.\n",
    "    \n",
    "    Parameters:\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    - n_members: S·ªë l∆∞·ª£ng node th√†nh vi√™n th·ª±c t·∫ø trong cluster (kh√¥ng t√≠nh cluster head)\n",
    "    \"\"\"\n",
    "    G, L = 100, 1024\n",
    "    P_t, P_r, P_idle, DR, DR_i = 1.6e-3, 0.8e-3, 0.1e-3, 4000, 1e6\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Member Node\n",
    "    E_tx_MN = G * P_t * L / DR\n",
    "    E_idle_MN = (best_time - G * L / DR) * P_idle\n",
    "    E_total_MN = E_tx_MN + E_idle_MN\n",
    "\n",
    "    # NƒÉng l∆∞·ª£ng cho Cluster Head (nh·∫≠n t·ª´ n_members node, truy·ªÅn cho AUV)\n",
    "    E_rx_TN = G * P_r * L * n_members / DR\n",
    "    E_tx_TN = G * P_t * L * n_members / DR_i\n",
    "    E_idle_TN = (best_time - (G*L*n_members/DR) - (G*L*n_members/DR_i)) * P_idle\n",
    "    E_total_TN = E_rx_TN + E_tx_TN + E_idle_TN\n",
    "\n",
    "    return {\n",
    "        \"Member\": {\"E_total\": E_total_MN},\n",
    "        \"Target\": {\"E_total\": E_total_TN}\n",
    "    }\n",
    "\n",
    "def update_energy(all_nodes, clusters, best_time):\n",
    "    \"\"\"\n",
    "    C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng cho t·∫•t c·∫£ c√°c node d·ª±a tr√™n s·ªë member th·ª±c t·∫ø c·ªßa t·ª´ng cluster.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary ch·ª©a th√¥ng tin t·∫•t c·∫£ c√°c node\n",
    "    - clusters: Dictionary ch·ª©a th√¥ng tin c√°c cluster\n",
    "    - best_time: Th·ªùi gian ho√†n th√†nh chu k·ª≥ AUV\n",
    "    \"\"\"\n",
    "    for cid, cinfo in clusters.items():\n",
    "        ch = cinfo.get('cluster_head')\n",
    "        nodes = cinfo.get('nodes', [])\n",
    "        \n",
    "        # T√≠nh s·ªë member nodes (kh√¥ng t√≠nh cluster head)\n",
    "        n_members = len([n for n in nodes if n != ch])\n",
    "        \n",
    "        # T√≠nh nƒÉng l∆∞·ª£ng cho cluster n√†y v·ªõi s·ªë member th·ª±c t·∫ø\n",
    "        energy_report = compute_energy(best_time, n_members)\n",
    "        \n",
    "        for nid in nodes:\n",
    "            if nid not in all_nodes: continue\n",
    "            if nid == ch:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Target']['E_total']\n",
    "            else:\n",
    "                all_nodes[nid]['residual_energy'] -= energy_report['Member']['E_total']\n",
    "            all_nodes[nid]['residual_energy'] = max(all_nodes[nid]['residual_energy'], 0.0)\n",
    "\n",
    "def remove_dead_nodes(all_nodes, clusters):\n",
    "    \"\"\"\n",
    "    Lo·∫°i b·ªè c√°c node ƒë√£ h·∫øt nƒÉng l∆∞·ª£ng v√† c·∫≠p nh·∫≠t l·∫°i clusters.\n",
    "    \n",
    "    Returns:\n",
    "    - new_clusters: Dictionary c√°c cluster c√≤n node s·ªëng\n",
    "    - dead: List c√°c node_id ƒë√£ ch·∫øt\n",
    "    \"\"\"\n",
    "    dead = [nid for nid, info in list(all_nodes.items()) if info['residual_energy'] <= 0]\n",
    "    for nid in dead:\n",
    "        del all_nodes[nid]\n",
    "\n",
    "    new_clusters = {}\n",
    "    for cid, cinfo in clusters.items():\n",
    "        alive_nodes = [nid for nid in cinfo.get('nodes', []) if nid in all_nodes]\n",
    "        if alive_nodes:\n",
    "            new_c = dict(cinfo)\n",
    "            new_c['nodes'] = alive_nodes\n",
    "            new_clusters[cid] = new_c\n",
    "\n",
    "    return new_clusters, dead\n",
    "\n",
    "def nearest_neighbor_path(centers):\n",
    "    \"\"\"\n",
    "    T√¨m ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t b·∫±ng thu·∫≠t to√°n Nearest Neighbor (Greedy TSP).\n",
    "    \n",
    "    Parameters:\n",
    "    - centers: List c√°c t·ªça ƒë·ªô c·∫ßn thƒÉm (b·∫Øt ƒë·∫ßu t·ª´ base station ·ªü index 0)\n",
    "    \n",
    "    Returns:\n",
    "    - path: List c√°c index theo th·ª© t·ª± thƒÉm\n",
    "    \"\"\"\n",
    "    n = len(centers)\n",
    "    if n == 1:\n",
    "        return [0]\n",
    "    unvisited = set(range(1, n))\n",
    "    path = [0]\n",
    "    current = 0\n",
    "    coords = np.array(centers)\n",
    "\n",
    "    while unvisited:\n",
    "        candidates = sorted(unvisited)\n",
    "        cand_coords = coords[candidates]\n",
    "        dists = np.linalg.norm(cand_coords - coords[current], axis=1)\n",
    "        next_idx = candidates[int(np.argmin(dists))]\n",
    "        path.append(next_idx)\n",
    "        unvisited.remove(next_idx)\n",
    "        current = next_idx\n",
    "\n",
    "    return path\n",
    "\n",
    "def recluster(all_nodes, node_positions, clustering_instance, r_sen=60, max_size=20, min_size=5):\n",
    "    \"\"\"\n",
    "    Ph√¢n c·ª•m l·∫°i to√†n b·ªô c√°c node c√≤n s·ªëng s·ª≠ d·ª•ng thu·∫≠t to√°n t·ª´ cluster.py.\n",
    "    \n",
    "    Parameters:\n",
    "    - all_nodes: Dictionary c√°c node c√≤n s·ªëng\n",
    "    - node_positions: Dictionary v·ªã tr√≠ c·ªßa c√°c node\n",
    "    - clustering_instance: Instance c·ªßa class Clustering\n",
    "    - r_sen: Ng∆∞·ª°ng kho·∫£ng c√°ch t·ªëi ƒëa trong c·ª•m\n",
    "    - max_size: S·ªë l∆∞·ª£ng node t·ªëi ƒëa trong 1 c·ª•m\n",
    "    - min_size: S·ªë l∆∞·ª£ng node t·ªëi thi·ªÉu trong 1 c·ª•m\n",
    "    \n",
    "    Returns:\n",
    "    - clusters: Dictionary c√°c c·ª•m m·ªõi\n",
    "    \"\"\"\n",
    "    ids = sorted(list(all_nodes.keys()))\n",
    "    if len(ids) == 0:\n",
    "        return {}\n",
    "\n",
    "    # T·∫°o m·∫£ng t·ªça ƒë·ªô nodes\n",
    "    coords = np.array([node_positions[nid] for nid in ids])\n",
    "    \n",
    "    # C·∫≠p nh·∫≠t tham s·ªë cho clustering instance\n",
    "    clustering_instance.r_sen = r_sen\n",
    "    clustering_instance.max_cluster_size = max_size\n",
    "    clustering_instance.min_cluster_size = min_size\n",
    "    \n",
    "    # Ph√¢n c·ª•m v·ªõi r√†ng bu·ªôc\n",
    "    clusters_data = clustering_instance.cluster_with_constraints(coords, ids)\n",
    "    \n",
    "    # Chuy·ªÉn ƒë·ªïi sang format c·∫ßn thi·∫øt\n",
    "    clusters = {}\n",
    "    for i, (cluster_nodes, cluster_ids) in enumerate(clusters_data):\n",
    "        center = np.mean(cluster_nodes, axis=0).tolist()\n",
    "        \n",
    "        # Ch·ªçn cluster head\n",
    "        ch = clustering_instance.choose_cluster_head(cluster_nodes, cluster_ids, all_nodes)\n",
    "        \n",
    "        clusters[i] = {\n",
    "            'nodes': cluster_ids,\n",
    "            'center': center,\n",
    "            'cluster_head': ch\n",
    "        }\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    H√†m ch√≠nh m√¥ ph·ªèng m·∫°ng c·∫£m bi·∫øn d∆∞·ªõi n∆∞·ªõc v·ªõi AUV thu th·∫≠p d·ªØ li·ªáu.\n",
    "    \"\"\"\n",
    "    input_dir = \"input_data\"\n",
    "    output_dir = \"result_ga_ch_most_energy\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(input_dir):\n",
    "        print(f\"‚ùå L·ªói: Th∆∞ m·ª•c {input_dir} kh√¥ng t·ªìn t·∫°i!\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(input_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if len(files) == 0:\n",
    "        print(f\"‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu n√†o trong {input_dir}\")\n",
    "        return\n",
    "\n",
    "    # Tham s·ªë\n",
    "    INITIAL_ENERGY = 100.0\n",
    "    v_f = 1.2\n",
    "    v_AUV = 3.0\n",
    "    R_SEN = 60      # B√°n k√≠nh truy·ªÅn t·∫£i (m)\n",
    "    MAX_SIZE = 20   # K√≠ch th∆∞·ªõc c·ª•m t·ªëi ƒëa\n",
    "    MIN_SIZE = 5    # K√≠ch th∆∞·ªõc c·ª•m t·ªëi thi·ªÉu\n",
    "    \n",
    "    results_summary = []\n",
    "    \n",
    "    # Kh·ªüi t·∫°o Clustering instance\n",
    "    clustering = Clustering(\n",
    "        space_size=400,\n",
    "        r_sen=R_SEN,\n",
    "        max_cluster_size=MAX_SIZE,\n",
    "        min_cluster_size=MIN_SIZE\n",
    "    )\n",
    "\n",
    "    for filename in files:\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"=== ƒêang x·ª≠ l√Ω file: {filename} ===\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            with open(input_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå L·ªói ƒë·ªçc file {filename}: {e}\")\n",
    "            continue\n",
    "\n",
    "        node_positions = {}\n",
    "        all_nodes = {}\n",
    "        \n",
    "        # X·ª≠ l√Ω file JSON - danh s√°ch nodes: [{\"id\": 0, \"x\": ..., \"y\": ..., \"z\": ...}, ...]\n",
    "        if isinstance(data, list):\n",
    "            print(\"ƒê·ªãnh d·∫°ng: Danh s√°ch nodes\")\n",
    "            for node in data:\n",
    "                nid = node['id']\n",
    "                all_nodes[nid] = {\n",
    "                    'initial_energy': node.get('initial_energy', INITIAL_ENERGY),\n",
    "                    'residual_energy': node.get('residual_energy', INITIAL_ENERGY)\n",
    "                }\n",
    "                node_positions[nid] = (node['x'], node['y'], node['z'])\n",
    "        else:\n",
    "            print(f\"‚ùå C·∫•u tr√∫c file {filename} kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ (c·∫ßn list of nodes)\")\n",
    "            continue\n",
    "\n",
    "        total_nodes = len(all_nodes)\n",
    "        print(f\"T·ªïng s·ªë node ban ƒë·∫ßu: {total_nodes}\")\n",
    "        print(f\"Tham s·ªë: r_sen={R_SEN}m, max_size={MAX_SIZE}, min_size={MIN_SIZE}\")\n",
    "\n",
    "        # Ph√¢n c·ª•m l·∫ßn ƒë·∫ßu ti√™n\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"PH√ÇN C·ª§M L·∫¶N ƒê·∫¶U TI√äN\")\n",
    "        print(f\"{'='*60}\")\n",
    "        initial_clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "        \n",
    "        # T√≠nh metrics cho ph√¢n c·ª•m ban ƒë·∫ßu\n",
    "        clusters_data_for_metrics = []\n",
    "        for cid, cinfo in initial_clusters.items():\n",
    "            cluster_nodes = np.array([node_positions[nid] for nid in cinfo['nodes']])\n",
    "            clusters_data_for_metrics.append((cluster_nodes, cinfo['nodes']))\n",
    "        \n",
    "        metrics = clustering.calculate_metrics(clusters_data_for_metrics)\n",
    "        \n",
    "        print(f\"\\n METRICS PH√ÇN C·ª§M:\")\n",
    "        print(f\"  - S·ªë c·ª•m: {metrics['num_clusters']}\")\n",
    "        print(f\"  - K√≠ch th∆∞·ªõc TB: {metrics['avg_cluster_size']:.1f}\")\n",
    "        print(f\"  - K√≠ch th∆∞·ªõc: [{metrics['min_cluster_size']} - {metrics['max_cluster_size']}]\")\n",
    "        print(f\"  - Kho·∫£ng c√°ch TB trong c·ª•m: {metrics['avg_intra_distance']:.1f}m\")\n",
    "        print(f\"  - Kho·∫£ng c√°ch max trong c·ª•m: {metrics['max_intra_distance']:.1f}m\")\n",
    "        print(f\"  - ƒê·ªô c√¢n b·∫±ng: {metrics['balance_score']:.2%}\")\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£ ph√¢n c·ª•m l·∫ßn ƒë·∫ßu\n",
    "        clusters_output = {}\n",
    "        for cid, cinfo in initial_clusters.items():\n",
    "            clusters_output[cid] = {\n",
    "                'cluster_id': cid,\n",
    "                'nodes': cinfo['nodes'],\n",
    "                'center': cinfo['center'],\n",
    "                'cluster_head': cinfo['cluster_head'],\n",
    "                'num_nodes': len(cinfo['nodes'])\n",
    "            }\n",
    "        \n",
    "        initial_cluster_file = os.path.join(output_dir, f\"initial_clusters_{filename}\")\n",
    "        with open(initial_cluster_file, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(clusters_output, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\n ƒê√£ l∆∞u ph√¢n c·ª•m ban ƒë·∫ßu: {initial_cluster_file}\")\n",
    "\n",
    "        cycle = 0\n",
    "        alive_log = []\n",
    "        energy_log = []\n",
    "        cluster_count_log = []\n",
    "\n",
    "        # V√≤ng l·∫∑p m√¥ ph·ªèng\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\" B·∫ÆT ƒê·∫¶U M√î PH·ªéNG\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        while True:\n",
    "            cycle += 1\n",
    "            alive_log.append(len(all_nodes))\n",
    "            total_energy = sum(all_nodes[n]['residual_energy'] for n in all_nodes)\n",
    "            energy_log.append(total_energy)\n",
    "\n",
    "            alive_ratio = len(all_nodes)/total_nodes if total_nodes > 0 else 0\n",
    "            \n",
    "            if alive_ratio < 0.9:\n",
    "                print(f\"\\nüõë D·ª´ng m√¥ ph·ªèng ·ªü cycle {cycle}: < 90% node c√≤n s·ªëng ({alive_ratio*100:.2f}%)\")\n",
    "                break\n",
    "\n",
    "            # Ph√¢n c·ª•m l·∫°i\n",
    "            clusters = recluster(all_nodes, node_positions, clustering, R_SEN, MAX_SIZE, MIN_SIZE)\n",
    "            cluster_count_log.append(len(clusters))\n",
    "            \n",
    "            if len(clusters) == 0:\n",
    "                print(f\"\\n D·ª´ng m√¥ ph·ªèng ·ªü cycle {cycle}: Kh√¥ng c√≤n node\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Cycle {cycle} --- | Alive: {alive_ratio*100:.2f}% ({len(all_nodes)}/{total_nodes}) | Energy: {total_energy:.2f}J | Clusters: {len(clusters)}\")\n",
    "\n",
    "            # T·∫°o ƒë∆∞·ªùng ƒëi cho AUV\n",
    "            sorted_keys = sorted(clusters.keys())\n",
    "            centers = [(0, 0, 0)]  # Base station\n",
    "            for k in sorted_keys:\n",
    "                centers.append(tuple(clusters[k]['center']))\n",
    "\n",
    "            path_indices = nearest_neighbor_path(centers)\n",
    "            best_time = travel_time(path_indices, centers, v_f, v_AUV)\n",
    "            \n",
    "            # C·∫≠p nh·∫≠t nƒÉng l∆∞·ª£ng\n",
    "            update_energy(all_nodes, clusters, best_time)\n",
    "            clusters, dead_nodes = remove_dead_nodes(all_nodes, clusters)\n",
    "            \n",
    "            if dead_nodes:\n",
    "                print(f\"   ‚ö° {len(dead_nodes)} node(s) ƒë√£ h·∫øt nƒÉng l∆∞·ª£ng\")\n",
    "\n",
    "        # L∆∞u k·∫øt qu·∫£ JSON\n",
    "        meta = {\n",
    "            'input_file': filename,\n",
    "            'initial_total_nodes': total_nodes,\n",
    "            'cycles_completed': cycle - 1,\n",
    "            'final_alive_nodes': len(all_nodes),\n",
    "            'final_alive_ratio': len(all_nodes)/total_nodes if total_nodes > 0 else 0,\n",
    "            'parameters': {\n",
    "                'r_sen': R_SEN,\n",
    "                'max_cluster_size': MAX_SIZE,\n",
    "                'min_cluster_size': MIN_SIZE,\n",
    "                'v_flow': v_f,\n",
    "                'v_AUV': v_AUV\n",
    "            },\n",
    "            'initial_clustering_metrics': {\n",
    "                'num_clusters': metrics['num_clusters'],\n",
    "                'avg_cluster_size': float(metrics['avg_cluster_size']),\n",
    "                'balance_score': float(metrics['balance_score'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        output_json = os.path.join(output_dir, f\"result_{filename}\")\n",
    "        with open(output_json, \"w\") as f:\n",
    "            json.dump(meta, f, indent=4)\n",
    "\n",
    "        results_summary.append((filename, cycle - 1))\n",
    "        print(f\"\\n File {filename}: {cycle - 1} cycles ho√†n th√†nh\")\n",
    "\n",
    "        # Plot 1: Alive nodes per cycle\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(alive_log)), alive_log, marker='o', linewidth=2, color='steelblue')\n",
    "        plt.title(f\"S·ªë node s·ªëng theo chu k·ª≥ - {filename}\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Chu k·ª≥\", fontsize=12)\n",
    "        plt.ylabel(\"Nodes alive\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axhline(y=total_nodes*0.9, color='red', linestyle='--', linewidth=2, label='Ng∆∞·ª°ng 90%')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"alive_nodes_{filename}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 2: Total energy per cycle\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(energy_log)), energy_log, marker='s', linewidth=2, color='orange')\n",
    "        plt.title(f\"NƒÉng l∆∞·ª£ng to√†n m·∫°ng theo chu k·ª≥ - {filename}\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Chu k·ª≥\", fontsize=12)\n",
    "        plt.ylabel(\"Total energy (J)\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"energy_{filename}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot 3: Number of clusters per cycle\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, len(cluster_count_log)+1), cluster_count_log, marker='^', linewidth=2, color='green')\n",
    "        plt.title(f\"S·ªë c·ª•m theo chu k·ª≥ - {filename}\", fontsize=14, fontweight='bold')\n",
    "        plt.xlabel(\"Chu k·ª≥\", fontsize=12)\n",
    "        plt.ylabel(\"S·ªë c·ª•m\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"clusters_{filename}.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "    # Summary plot\n",
    "    if results_summary:\n",
    "        labels = [x[0] for x in results_summary]\n",
    "        values = [x[1] for x in results_summary]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(labels, values, marker='o', linewidth=2, markersize=8)\n",
    "        plt.title(\"AUV cycles completed per dataset\", fontsize=14, fontweight='bold')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel(\"Cycles\", fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"summary_cycles.png\"), dpi=150)\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úÖ Ho√†n th√†nh! K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {output_dir}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    else:\n",
    "        print(\"\\n  Kh√¥ng c√≥ k·∫øt qu·∫£ n√†o ƒë∆∞·ª£c t·∫°o ra\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8644691,
     "sourceId": 13604032,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
